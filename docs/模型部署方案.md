# TestWings 模型部署方案（完全本地部署优先）

## 一、部署策略

### 1.1 核心原则

**优先完全本地部署：**
- 所有 AI 模型部署在手机本地
- 完成完整的用例分析、执行、图像识别和结果输出
- 降低测试环境复杂度，无需搭建服务器
- 保护数据隐私，不上传敏感信息

**企业级场景可选混合部署：**
- 针对企业级大规模使用场景
- 可单独搭建 LLM 模型服务器
- 支持云端和本地混合部署模式

### 1.2 整体架构

#### 完全本地部署架构（优先方案）

```
┌─────────────────────────────────────────────────────────┐
│               TestWings APP (手机端)                      │
│  ┌───────────────────────────────────────────────────┐ │
│  │           完全本地 AI 模型层                       │ │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐        │ │
│  │  │  OCR     │  │ UI检测   │  │ 用例理解 │        │ │
│  │  │ PaddleOCR│  │ YOLOv8   │  │ 本地LLM  │        │ │
│  │  │ Mobile   │  │ Nano     │  │ (量化)   │        │ │
│  │  └──────────┘  └──────────┘  └──────────┘        │ │
│  │  ┌──────────┐  ┌──────────┐                      │ │
│  │  │ 屏幕理解 │  │ 操作规划 │                      │ │
│  │  │ Vision-  │  │ 本地LLM  │                      │ │
│  │  │ Language │  │ (量化)   │                      │ │
│  │  └──────────┘  └──────────┘                      │ │
│  └───────────────────────────────────────────────────┘ │
│                                                        │
│  所有功能完全本地执行，无需网络连接                      │
└─────────────────────────────────────────────────────────┘
```

#### 混合部署架构（企业级可选方案）

```
┌─────────────────────────────────────────────────────────┐
│               TestWings APP (手机端)                      │
│  ┌───────────────────────────────────────────────────┐ │
│  │           本地轻量级模型层                          │ │
│  │  ┌──────────┐  ┌──────────┐                      │ │
│  │  │  OCR     │  │ UI检测   │                      │ │
│  │  │ PaddleOCR│  │ YOLOv8   │                      │ │
│  │  │ Mobile   │  │ Nano     │                      │ │
│  │  └──────────┘  └──────────┘                      │ │
│  └───────────────────────────────────────────────────┘ │
│                        ↕ HTTP/WebSocket                  │
│  ┌───────────────────────────────────────────────────┐ │
│  │           企业级 LLM 服务器                        │ │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐        │ │
│  │  │ 用例理解 │  │ 屏幕理解 │  │ 操作规划 │        │ │
│  │  │ LLM      │  │ Vision-  │  │ LLM      │        │ │
│  │  │          │  │ Language │  │          │        │ │
│  │  └──────────┘  └──────────┘  └──────────┘        │ │
│  └───────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────┘
```

### 1.3 模型分工（完全本地部署）

| 任务 | 模型类型 | 部署位置 | 内存占用 | 推理时间 | 模型大小 |
|------|---------|---------|---------|---------|---------|
| 文字识别 | OCR | 本地 | ~100MB | < 1s | ~20MB |
| UI元素检测 | 目标检测 | 本地 | ~200MB | < 1s | ~10MB |
| 用例理解 | 量化LLM | 本地 | ~4-6GB | 2-5s | ~4-7GB |
| 屏幕语义理解 | 量化Vision-Language | 本地 | ~4-6GB | 3-8s | ~4-7GB |
| 操作规划 | 量化LLM | 本地 | ~4-6GB | 1-3s | ~4-7GB |

**注意**：用例理解、屏幕理解、操作规划可以共享同一个 LLM 模型。

---

## 二、本地模型部署方案

### 2.1 OCR 模型：PaddleOCR Mobile

#### 模型信息
- **模型名称**：PaddleOCR Mobile
- **模型大小**：约 10-20MB
- **内存占用**：约 50-100MB（运行时）
- **推理速度**：
  - CPU：< 1秒（单张图片）
  - NPU：< 0.5秒（如果支持）

#### 集成方式

```kotlin
// 1. 添加依赖（build.gradle）
dependencies {
    implementation 'com.github.PaddlePaddle:PaddleOCR:latest'
    // 或使用本地 AAR
    implementation files('libs/paddleocr.aar')
}

// 2. 初始化 OCR
class OCRManager {
    private var ocrEngine: PaddleOCR? = null
    
    fun init(context: Context) {
        ocrEngine = PaddleOCR(context)
        ocrEngine?.init()
    }
    
    fun recognize(bitmap: Bitmap): List<TextElement> {
        val results = ocrEngine?.recognize(bitmap)
        return results?.map { result ->
            TextElement(
                text = result.text,
                bbox = BoundingBox(
                    x = result.box.left,
                    y = result.box.top,
                    width = result.box.width(),
                    height = result.box.height()
                ),
                confidence = result.confidence
            )
        } ?: emptyList()
    }
}
```

#### 模型文件结构
```
app/src/main/assets/models/ocr/
├── det_model.nb      # 文本检测模型
├── rec_model.nb      # 文本识别模型
└── cls_model.nb      # 文本方向分类模型（可选）
```

---

### 2.2 UI 元素检测模型：YOLOv8 Nano

#### 模型信息
- **模型名称**：YOLOv8 Nano
- **模型大小**：约 5-10MB
- **内存占用**：约 100-200MB（运行时）
- **推理速度**：
  - CPU：< 1秒（单张图片）
  - NPU：< 0.3秒（如果支持）

#### 检测类别
- Button（按钮）
- Input（输入框）
- Image（图片）
- Text（文本区域）
- Icon（图标）
- Checkbox（复选框）
- RadioButton（单选按钮）
- Switch（开关）

#### 集成方式

```kotlin
// 1. 使用 TensorFlow Lite
dependencies {
    implementation 'org.tensorflow:tensorflow-lite:2.14.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.14.0' // GPU 加速（可选）
}

// 2. UI 检测管理器
class UIDetector {
    private var interpreter: Interpreter? = null
    
    fun init(context: Context) {
        val modelFile = loadModelFile(context, "yolov8n_ui.tflite")
        val options = Interpreter.Options()
        options.setNumThreads(4) // 使用 4 个线程
        options.setUseNNAPI(true) // 使用 NNAPI（NPU 加速）
        interpreter = Interpreter(modelFile, options)
    }
    
    fun detect(bitmap: Bitmap): List<UIElement> {
        // 预处理图片
        val inputBuffer = preprocessImage(bitmap)
        
        // 推理
        val outputBuffer = ByteBuffer.allocateDirect(...)
        interpreter?.run(inputBuffer, outputBuffer)
        
        // 后处理：NMS、坐标转换
        val detections = postprocess(outputBuffer)
        
        return detections.map { detection ->
            UIElement(
                type = detection.className,
                bbox = detection.bbox,
                confidence = detection.confidence
            )
        }
    }
}
```

---

### 2.3 大语言模型（LLM）：量化模型

#### 模型选择

**推荐模型（按优先级）：**

1. **Qwen-7B-Chat（INT8 量化）** ⭐ 推荐
   - **模型大小**：约 4-5GB（INT8 量化）
   - **内存占用**：约 4-6GB（运行时）
   - **推理速度**：
     - CPU：5-10秒
     - NPU：2-5秒（如果支持）
   - **用途**：用例理解、操作规划、屏幕语义理解

2. **Qwen-1.8B-Chat（INT8 量化）** - 轻量级选择
   - **模型大小**：约 1-2GB（INT8 量化）
   - **内存占用**：约 1.5-2.5GB（运行时）
   - **推理速度**：
     - CPU：2-4秒
     - NPU：< 2秒（如果支持）
   - **用途**：简单用例理解、操作规划

3. **Llama-2-7B-Chat（INT8 量化）**
   - **模型大小**：约 4-5GB（INT8 量化）
   - **内存占用**：约 4-6GB（运行时）
   - **推理速度**：类似 Qwen-7B

#### 模型量化

**INT8 量化优势：**
- 模型大小减少 75%
- 内存占用减少 75%
- 推理速度提升 2-4 倍
- 准确率损失 < 5%

**量化工具：**
- ONNX Quantization
- TensorFlow Lite Quantization
- PaddlePaddle Quantization

#### 集成方式

**使用 ONNX Runtime：**

```kotlin
dependencies {
    implementation 'com.microsoft.onnxruntime:onnxruntime-android:1.16.0'
}

class LLMManager {
    private var ortEnv: OrtEnvironment? = null
    private var ortSession: OrtSession? = null
    
    fun init(context: Context) {
        ortEnv = OrtEnvironment.getEnvironment()
        val modelBytes = loadModel(context, "qwen_7b_int8.onnx")
        val sessionOptions = OrtSession.SessionOptions()
        
        // 配置优化选项
        sessionOptions.setIntraOpNumThreads(4) // 使用 4 个线程
        // sessionOptions.addExecutionProvider("NNAPI") // 使用 NPU（如果支持）
        
        ortSession = ortEnv?.createSession(modelBytes, sessionOptions)
    }
    
    suspend fun generate(
        prompt: String,
        maxTokens: Int = 512,
        temperature: Float = 0.7f
    ): String {
        // Tokenize
        val tokens = tokenize(prompt)
        
        // 推理
        val inputs = mapOf("input_ids" to tokens)
        val outputs = ortSession?.run(inputs)
        
        // Decode
        return decode(outputs)
    }
}
```

#### 模型文件结构
```
app/src/main/assets/models/llm/
├── qwen_7b_int8.onnx          # 量化后的 LLM 模型（约 4-5GB）
├── tokenizer.json             # Tokenizer 配置
└── config.json                # 模型配置
```

---

### 2.4 视觉-语言模型（Vision-Language Model）

#### 模型选择

**推荐模型：**

1. **Qwen-VL-Chat（INT8 量化）** ⭐ 推荐
   - **模型大小**：约 4-5GB（INT8 量化）
   - **内存占用**：约 4-6GB（运行时）
   - **推理速度**：
     - CPU：5-10秒
     - NPU：2-5秒（如果支持）
   - **用途**：屏幕语义理解、图像分析

2. **LLaVA-7B（INT8 量化）**
   - **模型大小**：约 4-5GB（INT8 量化）
   - **内存占用**：约 4-6GB（运行时）
   - **推理速度**：类似 Qwen-VL

#### 集成方式

```kotlin
class VisionLanguageModel {
    private var ortSession: OrtSession? = null
    
    fun init(context: Context) {
        val modelBytes = loadModel(context, "qwen_vl_int8.onnx")
        val sessionOptions = OrtSession.SessionOptions()
        ortSession = ortEnv?.createSession(modelBytes, sessionOptions)
    }
    
    suspend fun understandScreen(
        screenshot: Bitmap,
        textElements: List<TextElement>,
        uiElements: List<UIElement>
    ): ScreenState {
        // 预处理图片
        val imageTensor = preprocessImage(screenshot)
        
        // 构建提示词
        val prompt = buildScreenPrompt(textElements, uiElements)
        val textTensor = tokenize(prompt)
        
        // 推理
        val inputs = mapOf(
            "image" to imageTensor,
            "input_ids" to textTensor
        )
        val outputs = ortSession?.run(inputs)
        
        // 解析结果
        return parseScreenState(outputs)
    }
}
```

---

## 三、模型优化策略

### 3.1 NPU 加速（华为设备）

**使用 HiAI 框架或 NNAPI：**

```kotlin
class NPUAccelerator {
    fun optimizeForNPU(modelPath: String): String {
        // 使用 HiAI 转换模型
        val converter = HiAIModelConverter()
        val optimizedModel = converter.convert(
            modelPath = modelPath,
            targetDevice = "NPU",
            quantization = "INT8"
        )
        return optimizedModel
    }
}

// 在 ONNX Runtime 中使用 NNAPI
val sessionOptions = OrtSession.SessionOptions()
sessionOptions.addExecutionProvider("NNAPI") // 启用 NPU 加速
```

**NPU 加速效果：**
- 推理速度提升 5-10 倍
- 降低 CPU 负载和功耗
- 支持 INT8 量化模型

### 3.2 模型缓存与预加载

```kotlin
class ModelManager {
    private val modelCache = mutableMapOf<String, Any>()
    
    fun preloadModels(context: Context) {
        // 在后台线程预加载模型
        GlobalScope.launch(Dispatchers.IO) {
            // 预加载 OCR 模型（轻量级，优先加载）
            loadOCRModel(context)
            
            // 预加载 UI 检测模型
            loadUIDetectorModel(context)
            
            // 延迟加载 LLM（较大，按需加载）
            // loadLLMModel(context)
        }
    }
    
    fun loadLLMModelOnDemand(context: Context) {
        if (!modelCache.containsKey("llm")) {
            GlobalScope.launch(Dispatchers.IO) {
                loadLLMModel(context)
            }
        }
    }
    
    private fun hasEnoughMemory(): Boolean {
        val runtime = Runtime.getRuntime()
        val freeMemory = runtime.freeMemory()
        val totalMemory = runtime.totalMemory()
        val availableMemory = runtime.maxMemory() - (totalMemory - freeMemory)
        
        // 需要至少 6GB 可用内存才加载 LLM
        return availableMemory > 6 * 1024 * 1024 * 1024
    }
}
```

### 3.3 模型共享优化

**用例理解、操作规划、屏幕理解可以共享同一个 LLM：**

```kotlin
class SharedLLMManager {
    private var llmModel: OrtSession? = null
    
    fun init(context: Context) {
        // 只加载一个 LLM 模型
        llmModel = loadLLMModel(context, "qwen_7b_int8.onnx")
    }
    
    suspend fun understandTestCase(testCase: String): TestPlan {
        val prompt = buildTestCasePrompt(testCase)
        return generate(prompt, taskType = "understand")
    }
    
    suspend fun planOperation(step: TestStep, screen: ScreenState): OperationPlan {
        val prompt = buildOperationPrompt(step, screen)
        return generate(prompt, taskType = "plan")
    }
    
    suspend fun understandScreen(screenshot: Bitmap): ScreenState {
        val prompt = buildScreenPrompt(screenshot)
        return generate(prompt, taskType = "understand_screen")
    }
    
    private suspend fun generate(prompt: String, taskType: String): Any {
        // 使用同一个模型，通过不同的 prompt 实现不同功能
        val tokens = tokenize(prompt)
        val inputs = mapOf("input_ids" to tokens)
        val outputs = llmModel?.run(inputs)
        return parseOutput(outputs, taskType)
    }
}
```

---

## 四、硬件配置要求

### 4.1 完全本地部署配置要求

#### 最低配置（8GB 内存）
- **内存**：8GB RAM
- **存储**：64GB+（用于存储模型文件）
- **处理器**：支持 NPU 推荐

**可部署模型：**
- OCR 模型：✅
- UI 检测模型：✅
- Qwen-1.8B（INT8）：✅（约 1.5-2.5GB 内存）
- Qwen-7B（INT8）：⚠️（可能内存不足，需要优化）

**总内存占用估算：**
- 系统：~2-3GB
- OCR：~100MB
- UI 检测：~200MB
- LLM（1.8B）：~1.5-2.5GB
- APP 运行：~500MB
- **总计**：约 4.5-6.5GB（接近极限）

#### 推荐配置（12GB+ 内存）
- **内存**：12GB+ RAM（推荐 16GB）
- **存储**：128GB+（用于存储模型文件）
- **处理器**：支持 NPU（如麒麟 990/9000、骁龙 8 Gen 2+）

**可部署模型：**
- OCR 模型：✅
- UI 检测模型：✅
- Qwen-7B（INT8）：✅（约 4-6GB 内存）
- Qwen-VL（INT8）：✅（约 4-6GB 内存，与 LLM 共享）

**总内存占用估算：**
- 系统：~2-3GB
- OCR：~100MB
- UI 检测：~200MB
- LLM（7B）：~4-6GB
- APP 运行：~500MB
- **总计**：约 7-10GB（12GB 内存充足）

### 4.2 模型选择建议

**8GB 内存设备：**
- 使用 Qwen-1.8B（INT8）进行用例理解和操作规划
- 屏幕理解可以结合 OCR + UI 检测 + 规则引擎，不必须 Vision-Language 模型

**12GB+ 内存设备：**
- 使用 Qwen-7B（INT8）进行所有 LLM 任务
- 可选部署 Qwen-VL 进行复杂屏幕理解

---

## 五、企业级混合部署方案（可选）

### 5.1 适用场景

**企业级大规模使用：**
- 大量测试用例并发执行
- 需要集中管理和监控
- 需要模型统一更新和维护

### 5.2 架构设计

```
┌─────────────────────────────────────────────────────────┐
│              企业级 LLM 服务器                             │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │ 用例理解 │  │ 屏幕理解 │  │ 操作规划 │              │
│  │ LLM      │  │ Vision-  │  │ LLM      │              │
│  │          │  │ Language │  │          │              │
│  └──────────┘  └──────────┘  └──────────┘              │
└────────────────────┬────────────────────────────────────┘
                     │ HTTP/WebSocket
                     │
┌────────────────────▼────────────────────────────────────┐
│           多个 TestWings APP (手机端)                    │
│  ┌──────────┐  ┌──────────┐                            │
│  │  OCR     │  │ UI检测   │                            │
│  │ PaddleOCR│  │ YOLOv8   │                            │
│  │ Mobile   │  │ Nano     │                            │
│  └──────────┘  └──────────┘                            │
└─────────────────────────────────────────────────────────┘
```

### 5.3 服务器部署

**硬件要求：**
- GPU：至少 1x NVIDIA A100 (40GB) 或 2x RTX 3090 (24GB)
- 内存：至少 32GB RAM
- 存储：至少 100GB（用于模型文件）

**部署方案：**
- 使用 vLLM 或 TensorRT-LLM 部署
- 提供 RESTful API 或 WebSocket 接口
- 支持并发请求和负载均衡

---

## 六、部署建议总结

### 6.1 优先方案：完全本地部署

**优势：**
- ✅ 降低测试环境复杂度，无需搭建服务器
- ✅ 保护数据隐私，不上传敏感信息
- ✅ 离线可用，不依赖网络
- ✅ 响应速度快，无网络延迟

**配置要求：**
- 8GB 内存：可以使用 Qwen-1.8B（轻量级）
- 12GB+ 内存：推荐使用 Qwen-7B（完整功能）

### 6.2 可选方案：企业级混合部署

**适用场景：**
- 企业级大规模使用
- 需要集中管理
- 需要统一更新模型

**架构：**
- 本地：OCR + UI 检测（轻量级模型）
- 云端：LLM 服务（大模型）

---

## 七、实施路线图

### Phase 1: 基础模型部署
- [ ] OCR 模型集成（PaddleOCR Mobile）
- [ ] UI 检测模型集成（YOLOv8 Nano）
- [ ] 基础功能验证

### Phase 2: LLM 本地部署
- [ ] 选择并量化 LLM 模型（Qwen-1.8B 或 Qwen-7B）
- [ ] 集成 ONNX Runtime
- [ ] 实现用例理解功能
- [ ] 实现操作规划功能

### Phase 3: Vision-Language 模型（可选）
- [ ] 集成 Qwen-VL 或 LLaVA
- [ ] 实现屏幕语义理解
- [ ] 优化推理性能

### Phase 4: 性能优化
- [ ] NPU 加速优化
- [ ] 模型缓存优化
- [ ] 内存管理优化

### Phase 5: 企业级支持（可选）
- [ ] 混合部署架构
- [ ] 服务器端 LLM 服务
- [ ] 负载均衡和监控
