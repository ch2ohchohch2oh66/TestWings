# TestWings 开发路线图

> **本文档是 TestWings 项目的唯一总体规划文档**，记录开发进度、已完成功能和下一步计划。其他文档中的Phase/规划内容已统一到此文档。

## 🎯 终极目标

**TestWings 是一个基于 AI 的 APP 自动化测试框架，能够：**

1. **理解测试用例**：支持自然语言描述，LLM自动理解并生成测试计划
2. **智能屏幕识别**：Vision-Language模型理解屏幕内容（VL为主，OCR为辅）
3. **智能元素定位**：基于VL模型语义匹配，自动定位目标元素，适应UI变化
4. **自动化执行**：纯视觉操作，执行点击、输入、滑动等操作（纯坐标）
5. **智能结果验证**：自动验证测试结果，生成详细测试报告
6. **AI自主决策**：LLM根据用例和屏幕状态自主决策操作

**核心原理**：Vision-Language模型屏幕理解 + LLM自主决策 + 纯视觉操作执行

---

## ✅ 已完成功能

### Phase 0: 项目基础（已完成）
- [x] 项目结构搭建
- [x] Android 开发环境配置
- [x] 基础 UI 界面（Jetpack Compose）
- [x] **屏幕捕获功能** ⭐
  - [x] MediaProjection API 集成
  - [x] 前台服务实现（兼容 HarmonyOS）
  - [x] 截图保存到本地
  - [x] 权限管理
  - [x] MediaProjection 实例复用优化
  - [x] 授权弹窗延迟优化（从 3.5 秒优化到 0-500ms）

### Phase 1: 核心功能（MVP）- 进行中

#### 1.1 基础操作执行 ⭐ **已完成**
**目标**：实现能够操作手机的基础能力（纯坐标操作）

**任务清单：**
- [x] 实现点击操作
  - [x] 根据坐标点击（纯坐标操作）
  - [x] 点击事件分发
- [x] 实现输入操作
  - [x] 文本输入
  - [x] 清空输入框
  - [x] 特殊字符处理（基础支持）
- [x] 实现滑动操作
  - [x] 上下滑动
  - [x] 左右滑动
  - [x] 自定义滑动路径
- [x] 实现系统按键
  - [x] 返回键
  - [x] 主页键
  - [x] 最近任务键
- [x] 测试基本操作流程
  - [x] 创建测试界面
  - [x] 验证点击功能 ✅
  - [x] 验证滑动功能 ✅
  - [x] 验证返回键功能 ✅

**完成时间**：2025-01-08

**测试状态**：✅ 已测试通过
- ✅ 屏幕捕获功能正常
- ✅ 上下左右滑动操作正常
- ✅ 点击中心操作正常
- ✅ 返回键操作正常

**注意**：采用纯视觉AI方案，不依赖AccessibilityService，所有操作基于坐标。

---

## 🚀 下一步计划

### Phase 1: 核心功能（MVP）- 进行中

#### 1.2 OCR 文字识别 ⭐ **已完成**（降级方案）
**目标**：实现OCR识别功能，作为VL模型的降级方案

**完成时间**：2025-12-09

**任务清单：**
- [x] 选择 OCR 库（使用 ML Kit Text Recognition）
- [x] 集成 OCR 库到项目
- [x] 实现截图 OCR 识别功能
- [x] 提取文字内容和位置信息
- [x] 显示识别结果（用于测试和调试）
- [x] 优化识别性能（异步处理）
- [x] 实现自动检测和选择机制（HarmonyOS/Android 自动适配）

**技术选型**：
- **ML Kit Text Recognition**（已采用，作为降级方案）
  - ✅ 轻量级、易集成、免费、支持中文
  - ✅ 已验证在 HarmonyOS 4.2 上可用
  - ⚠️ 识别准确率有限，主要用于降级场景

**定位**：
- ⚠️ **VL为主，OCR为辅**：OCR仅作为VL模型不可用时的降级方案
- ⚠️ 后续将集成Vision-Language模型作为主要能力

**测试状态**：✅ 已测试通过（基础功能）
- ✅ OCR识别功能正常
- ✅ 能够识别屏幕上的文字
- ✅ 能够提取文本块的位置信息（坐标）

---

#### 1.3 Vision-Language模型集成 ⭐ **下一步重点**
**目标**：集成Vision-Language模型，实现屏幕理解能力（主要能力）

**技术选型**：
- **模型**：Qwen2-VL-2B-Instruct（Q4F16量化，推荐）⭐
  - **解码器**：`decoder_model_merged_q4f16.onnx`（约829MB）
  - **视觉编码器**：`vision_encoder_q4f16.onnx`（约1.27GB）
  - **总大小**：约2.1GB
  - **注意**：INT8量化版本使用ConvInteger操作符，ONNX Runtime Android不支持，因此使用Q4F16版本
- **推理框架**：ONNX Runtime for Android (1.20.0)
- **模型位置**：外部存储 `/sdcard/Android/data/com.testwings/files/models/vl/`（避免APK过大）
- **获取方式**：手动下载（从Hugging Face）或使用download_model.py脚本
- **文件识别**：APP自动识别任何.onnx文件，按优先级使用（Q4F16 > 未量化 > INT8）

**详细任务清单**：

**第一阶段：基础准备** ✅ **已完成**
- [x] 添加ONNX Runtime依赖到build.gradle.kts
- [x] 准备Qwen2-VL-2B-Instruct模型文件（INT8量化）和配置文件，放置到外部存储（/sdcard/Android/data/com.testwings/files/models/vl/）
  - [x] 创建ModelDownloader类（支持从Hugging Face下载）
  - [x] 更新VisionLanguageManager（从外部存储加载模型，支持自动识别.onnx文件）
  - [x] 更新文档（Qwen2-VL模型部署操作手册）
- [x] 创建ScreenState数据模型（包含elements、semanticDescription等）
- [x] 创建UIElement数据模型（type、text、bounds、center、confidence等）

**第二阶段：核心实现** ⏳ **进行中（双路径并行）**

**路径A：本地部署（保留，等待ONNX Runtime修复）** ⏳
- [x] 创建VisionLanguageManager类，实现模型加载和初始化框架
- [x] 实现VL模型推理接口框架（understand方法，输入Bitmap，输出ScreenState）
- [x] 实现vision_encoder模型加载和推理逻辑（图像预处理、视觉编码器推理）✅ **已完成**
  - [x] 修复vision_encoder输入格式（patch转换和grid_thw计算）✅ **已完成**
  - [x] 成功加载Q4F16版本的视觉编码器（1270MB）和解码器（829MB）✅ **已验证**
  - [x] 修复模型优先级逻辑（Q4F16优先，INT8自动跳过）✅ **已完成**
- [x] 测试vision_encoder推理（调用runVisionEncoder验证输出）✅ **已完成（2025-12-15）**
  - [x] 修复图像尺寸问题（从960x960改为448x448，根据README.md）
  - [x] 修复temporal_patch_size（从1改为2，模型期望）
  - [x] 修复pixel_values格式（扁平化为[1024, 1176]，模型内部reshape为[1024, 3, 2, 14, 14]）
  - [x] 验证grid_thw格式（[1, 3] = [[1, 32, 32]]）
  - [x] 成功完成推理，输出形状[256, 1536]，耗时~18秒
- [ ] 实现文本tokenization和嵌入（tokenizer.json使用、文本嵌入生成）⏳ **等待ONNX Runtime修复**
- [ ] 实现decoder推理逻辑（合并嵌入、inputs_embeds处理、logits生成）⏳ **等待ONNX Runtime修复**
- [ ] 实现VL模型输出解析（logits解码、JSON解析、转换为ScreenState结构）⏳ **等待ONNX Runtime修复**
- ⚠️ **当前状态**：被ONNX Runtime zero dimension限制阻塞（Issue #26841已提交，等待官方回复）

**路径B：API方式（新增，立即开发）** ⭐ **当前重点**
- [ ] 设计统一接口（IVisionLanguageService）⭐ **下一步优先**
- [ ] 重构本地实现为LocalVisionLanguageService（保留现有代码）
- [ ] 实现APIVisionLanguageService（统一处理云端和局域网API）
- [ ] 实现统一管理器VisionLanguageManager（支持运行时切换）
- [ ] 配置API服务（优先使用云端API进行快速验证）

**第三阶段：系统集成**
- [ ] 更新ElementLocator，支持VL模型定位（优先使用VL，降级到OCR）
- [ ] 更新SemanticMatcher，支持VL模型语义匹配（替代OCR匹配）
- [ ] 更新TestExecutor，集成VL模型到测试执行流程

**第四阶段：优化和测试**
- [ ] 实现模型加载状态管理（加载中、已加载、加载失败等）
- [ ] 实现模型按需加载和内存管理（避免内存溢出）
- [ ] 添加VL模型可用性检查（模型文件是否存在、是否已加载）
- [ ] 实现错误处理和降级机制（VL失败时自动降级到OCR）
- [ ] 优化推理性能（异步处理、缓存机制、NPU加速支持）
- [ ] 创建VL模型测试用例（验证识别准确性和性能）
- [ ] 集成VL模型到MainActivity（UI显示、状态管理）

**预计时间**：10-15 天

#### 1.4 LLM集成（用例理解和操作规划）
**目标**：集成LLM，实现用例理解和AI自主决策

**任务清单：**
- [ ] 选择LLM模型（Qwen-7B-Chat，INT8量化）
- [ ] 集成LLM到项目
- [ ] 实现用例理解功能
- [ ] 实现操作规划功能
- [ ] AI自主决策能力

**预计时间**：10-15 天

#### 1.5 简单测试用例执行
**目标**：能够执行一个简单的测试用例（基于VL+LLM）

**任务清单：**
- [ ] 设计简单的测试用例格式（自然语言或JSON）
- [ ] LLM理解测试用例
- [ ] VL模型理解屏幕
- [ ] 执行测试步骤（纯视觉操作）
- [ ] 记录执行结果
- [ ] 生成简单报告

**预计时间**：5-7 天

---

#### 1.4 基础报告生成
**目标**：生成测试执行报告

**任务清单：**
- [ ] 设计报告格式
- [ ] 收集执行数据
- [ ] 生成 HTML/JSON 报告
- [ ] 包含截图和日志

**预计时间**：3-5 天

---

## 📋 后续阶段（Phase 2-4）

### Phase 2: AI 增强
- [ ] UI 元素检测（YOLOv8）
- [ ] 本地 LLM 集成（Qwen-1.8B）
- [ ] 用例理解功能
- [ ] 智能元素定位
- [ ] 操作规划

### Phase 3: 完善功能
- [ ] 复杂操作支持（手势、长按等）
- [ ] 结果验证增强
- [ ] 异常处理优化
- [ ] 报告完善

### Phase 4: 优化与扩展
- [ ] 性能优化
- [ ] 插件系统
- [ ] 多平台支持
- [ ] 企业级功能

---

## 🎯 当前里程碑

**当前阶段**：Phase 1 - 核心功能（MVP）

**当前任务**：Vision-Language模型集成 ⭐

**完成度**：约 75%
- ✅ 屏幕捕获（100%）
- ✅ 基础操作执行（100%）
  - ✅ 点击操作（坐标点击、元素点击）
  - ✅ 滑动操作（上下左右、自定义路径）
  - ✅ 输入操作（文本输入、清空）
  - ✅ 系统按键（返回、主页、最近任务）
- ✅ OCR 识别（85%）⭐ 基础功能已完成，准确性待优化
  - ✅ ML Kit Text Recognition 集成
  - ✅ 自动检测和选择机制（HarmonyOS/Android 适配）
  - ✅ OCR结果显示界面
  - ✅ 文本块位置信息提取
  - ⚠️ 识别准确性优化（灰底蓝字场景识别不准确）
- ✅ 用例执行（100%）⭐ 已完成
  - ✅ 测试用例格式设计（JSON）
  - ✅ 测试用例解析和执行
  - ✅ 截图和OCR集成
  - ✅ 验证功能
- ⏳ Vision-Language模型集成（50%）⭐ 下一步重点
  - ✅ 基础准备（依赖、模型文件、数据结构）- 已完成
  - ⏳ 核心实现（模型加载完成，vision_encoder推理已验证成功）- 进行中
    - ✅ 模型加载和初始化框架 - 已完成
    - ✅ vision_encoder输入格式修复（patch转换和grid_thw计算）- 已完成
    - ✅ vision_encoder推理测试 - **已完成（2025-12-15）** ⭐
      - ✅ 图像尺寸修复（448x448）
      - ✅ temporal_patch_size修复（2）
      - ✅ pixel_values格式验证（[1024, 1176]）
      - ✅ 推理成功，输出[256, 1536]，耗时~18秒
    - ⏳ 文本tokenization和嵌入 - 待实现
    - ⏳ decoder推理和输出解析 - 待实现
  - ⏳ 系统集成（ElementLocator、SemanticMatcher、TestExecutor）- 待开始
  - ⏳ 优化和测试（性能优化、错误处理、测试验证）- 待开始
- ⏳ OCR准确性优化（0%）- 已确定VL为主，OCR为辅，暂缓
- ⏳ 报告生成（0%）

---

## 📝 开发原则

1. **先做能用的，再做完美的**
   - MVP 版本优先实现核心功能
   - 后续逐步优化和完善

2. **循序渐进**
   - 每个功能独立实现和测试
   - 确保每个功能稳定后再继续

3. **本地优先**
   - 优先使用本地模型和本地处理
   - 降低环境复杂度

4. **文档同步**
   - 代码和文档同步更新
   - 记录开发过程中的问题和解决方案

---

## 🔄 更新记录

- **2025-01-08**：完成屏幕捕获功能，开始规划下一步
- **2025-01-08**：创建开发路线图文档
- **2025-01-08**：完成基础操作执行功能
  - ✅ 实现 AccessibilityService
  - ✅ 实现点击、滑动、输入、系统按键等操作
  - ✅ 创建操作测试界面和反馈机制
  - ✅ 所有基础操作测试通过
- **2025-01-08**：更新进度，确定下一步为 OCR 文字识别
- **2025-01-08**：优化屏幕捕获授权弹窗延迟
  - ✅ 优化授权弹窗显示延迟（从 3.5 秒优化到 0-500ms）
  - ✅ 添加智能服务状态检查（服务已运行时立即显示弹窗）
  - ✅ 减少固定延迟时间（前台服务启动通常很快）
  - ✅ 优化授权后捕获延迟（从 1.5 秒优化到 500ms）
- **2025-12-09**：完成 OCR 文字识别功能
  - ✅ 集成 ML Kit Text Recognition
  - ✅ 实现 OCR 识别功能（截图后自动识别）
  - ✅ 创建 OCR 结果显示界面（文本块详情、位置信息）
  - ✅ 实现自动检测和选择机制（HarmonyOS/Android 自动适配）
  - ✅ 验证 ML Kit 在 HarmonyOS 4.2 上可用（即使没有 Google Play Services）
  - ✅ 创建 OCR 功能验证报告文档
- **2025-01-11**：完成简单测试用例执行功能
  - ✅ 设计测试用例格式（JSON）
  - ✅ 实现测试用例解析器
  - ✅ 实现测试执行引擎（TestExecutor）
  - ✅ 实现元素定位器（ElementLocator）
  - ✅ 集成截图和OCR到测试执行流程
  - ✅ 修复截图重复问题（一次截图操作只产生一张截图）
  - ✅ 创建测试用例管理界面
- **2025-01-11**：发现 OCR 识别准确性问题
  - ⚠️ 灰底蓝字场景识别不准确（如"捕获屏幕"识别成"鋪获屏幕"或"埔获屏幕"）
  - 📋 制定优化方案：图像预处理增强、后处理纠错、多引擎融合
- **2025-01-11**：确定技术路线 - VL为主，OCR为辅
  - ✅ 从工程实践角度评估：VL模型是核心能力，必需集成
  - ✅ PaddleOCR可作为更好的降级方案（替代ML Kit），但不能替代VL
  - ✅ 制定VL模型集成详细TODO清单（18项任务，分4个阶段）
  - 📋 下一步：开始VL模型集成，预计10-15天完成
- **2025-12-14**：完成VL模型集成第一阶段（基础准备）
  - ✅ 模型选型确定：Qwen2-VL-2B-Instruct（从Qwen-VL-Chat改为更小的2B版本）
  - ✅ 模型文件部署方案：外部存储，避免APK过大
  - ✅ 创建VisionLanguageManager框架（模型检查、加载状态管理）
  - ✅ 实现模型文件自动识别（通过.onnx后缀，按优先级查找）
  - ✅ 创建ScreenState和UIElement数据模型
  - ✅ 完成模型部署文档（Qwen2-VL模型部署操作手册）
  - 📋 下一步：实现ONNX Runtime模型推理逻辑（图像预处理、模型加载、推理执行）
- **2025-12-14**：完成vision_encoder加载和基础推理实现
  - ✅ 添加vision_encoder模型文件查找和加载逻辑（支持INT8和Q4F16量化版本）
  - ✅ 实现图像预处理（调整到960x960，保持宽高比，归一化到[0,1]）
  - ✅ 实现vision_encoder推理框架（图像 → 图像嵌入向量）
  - ✅ 修复OutOfMemoryError（使用文件路径加载模型，避免一次性读取到内存）
  - ✅ 更新模型部署文档（添加vision_encoder下载说明）
  - ✅ 发现INT8量化模型不兼容问题（ConvInteger操作符不支持），改用Q4F16版本
  - ✅ 成功加载Q4F16版本的视觉编码器（1270MB）和解码器（829MB）✅ **已验证**
  - ✅ 确认视觉编码器输入格式：`pixel_values`（扁平化patch数据）和 `grid_thw`（网格信息）
  - ✅ 修复vision_encoder输入格式（实现patch转换和grid_thw计算，使用ceil确保正确）✅ **已完成**
  - ✅ 修复模型优先级逻辑（Q4F16优先，INT8版本自动跳过并提示错误）✅ **已完成**
  - ✅ 修复LongBuffer导入和grid_thw张量创建（使用LongBuffer.wrap包装LongArray）✅ **已完成**
- **2025-12-15**：完成vision_encoder推理测试 ✅ **测试成功**
  - ✅ 修复图像尺寸问题（从960x960改为448x448，根据README.md示例）
  - ✅ 修复temporal_patch_size（从1改为2，模型期望reshape为[-1, 3, 2, 14, 14]）
  - ✅ 修复pixel_values格式（扁平化为[num_patches, 1176]，模型内部reshape为[num_patches, 3, 2, 14, 14]）
  - ✅ 验证grid_thw格式（[1, 3] = [[grid_t, grid_h, grid_w]] = [[1, 32, 32]]）
  - ✅ 成功完成推理测试：输出形状[256, 1536]，推理耗时~18秒
  - ✅ 更新模型结构研究文档（添加已验证的输入输出格式和测试结果）
- **2025-12-21**：完成统一接口设计、API客户端实现和系统集成 ✅
  - ✅ 创建`IVisionLanguageService`统一接口
  - ✅ 创建`LocalVisionLanguageService`（本地部署实现）
  - ✅ 创建`ApiVisionLanguageService`（API方式实现，完整功能）
  - ✅ 创建`VisionLanguageServiceManager`统一管理器
  - ✅ 将`LoadState`枚举移到独立文件
  - ✅ 支持运行时切换部署方式（LOCAL / API_CLOUD / API_LOCAL / AUTO）
  - ✅ 实现自动降级机制（本地↔API，VL↔OCR）
  - ✅ 完善API响应解析（支持多种常见格式）
  - ✅ 集成到MainActivity（替换原有VisionLanguageManager）
  - ✅ 创建API配置示例文档
  - ✅ 更新ElementLocator，支持VL模型定位（使用统一接口）
  - ✅ 更新SemanticMatcher，添加VL模型语义匹配支持
  - ✅ 更新TestExecutor，集成VL模型到测试执行流程
  - ✅ 实现完整降级机制（VL失败时自动降级到OCR）
  - ✅ 修复编译错误，APP编译安装正常 ✅
  - ✅ 研究并整合LLM/VL API服务选型到模型部署方案文档 ✅
  - ✅ **最终方案确定**（详见 `01-03-模型部署方案.md` 第五章）：
    - **主方案**：通义千问（LLM: Qwen-Plus + VL: Qwen-VL）✅
    - **备选方案**：DeepSeek-V2（LLM）+ 通义千问VL（VL）✅
  - 📋 下一步：注册并配置API服务（通义千问），设计LLM统一接口
- **2025-12-20**：遇到ONNX Runtime Android zero dimension限制问题 ⚠️
  - ❌ 问题：ONNX Runtime Android不支持0维度tensor，导致decoder的past_key_values无法使用seq_len=0初始化
  - ❌ 影响：阻塞decoder推理实现，影响完整VL模型推理流程
  - ✅ 定位：通过系统性排查（方案1-10、示例代码对比、框架限制分析）定位到根本原因
  - ✅ Issue提交：已提交Issue到ONNX Runtime（#26841），等待官方回复
  - ✅ 文档整理：完成问题分析文档和方法论文档
  - ✅ 代码清理：清理代码中的调试注释和临时记录
  - 📋 下一步：监控Issue状态，同时实现非阻塞功能（文本tokenization）
  - 📋 详细方案：参见 `docs/01-03-模型部署方案.md` 第6节

---

**当前状态（2025-12-21）**：
- ✅ APP编译安装正常
- ✅ 优先级1-4全部完成（统一接口、API客户端、系统集成）
- ⏳ **下一步**：选择并配置LLM/VL API服务，开始LLM集成
- ✅ vision_encoder推理测试已完成
- ⚠️ **遇到ONNX Runtime Android zero dimension限制问题**
  - 问题：ONNX Runtime Android不支持0维度tensor，导致decoder的past_key_values无法正确初始化
  - 影响：阻塞本地部署decoder推理实现
  - 状态：问题已定位，Issue已提交（ONNX Runtime #26841），等待官方回复
  - 详细分析：参见 `docs/ONNX-Runtime-Android-zero-dimension限制导致Transformer模型past_key_values初始化失败问题分析.md`

**策略调整（2025-12-20）**：**双路径并行开发**（新方案实现于2025-12-21）
- **路径A（保留）**：本地部署VL实现，等待ONNX Runtime修复后继续
- **路径B（新增）**：开发基于API的VL识别，可以立即使用（云端或局域网）
- **目标**：DEMO PROJECT验证LLM+VL可行性，部署方式不是关键
- **详细方案**：参见 `docs/VL部署策略调整-本地+API双路径方案.md`

**下一步计划**（立即开始）：

**路径B：API方式开发** ⭐ **当前重点**

**短期（1-2周内）**：
- **优先级1**：设计统一接口（IVisionLanguageService）（1-2天）⭐ **已完成（2025-12-21）**
  - [x] 定义`IVisionLanguageService`接口 ✅
  - [x] 重构现有本地实现为`LocalVisionLanguageService` ✅
  - [x] 设计`APIVisionLanguageService`接口和配置结构 ✅
  - [x] 创建`VisionLanguageServiceManager`统一管理器 ✅
  - [x] 将`LoadState`枚举移到独立文件 ✅
- **优先级2**：实现API客户端（APIVisionLanguageService）（2-3天）⭐ **已完成（2025-12-21）**
  - [x] 实现`APIVisionLanguageService`类（基础框架） ✅
  - [x] 支持多种认证方式（API Key、Bearer Token等） ✅
  - [x] 实现图像压缩和上传 ✅
  - [x] 实现错误处理和重试机制 ✅
  - [x] 完善API响应解析（支持多种常见格式） ✅
  - [x] 创建API配置示例文档 ✅
  - [ ] 配置实际API服务进行验证 ⏳ **下一步（需要实际API端点）**
- **优先级3**：实现统一管理器（1天）⭐ **已完成（2025-12-21）**
  - [x] 实现`VisionLanguageServiceManager`统一管理器 ✅
  - [x] 支持配置选择部署方式（LOCAL / API_CLOUD / API_LOCAL / AUTO） ✅
  - [x] 实现自动降级机制（本地失败时降级到API，API失败时降级到本地） ✅
  - [x] 集成到现有代码（MainActivity） ✅

**中期（2-3周内）**：
- **优先级4**：系统集成（3-5天）⭐ **已完成（2025-12-21）**
  - [x] 更新ElementLocator，支持VL模型定位（使用统一接口） ✅
  - [x] 更新SemanticMatcher，支持VL模型语义匹配 ✅
  - [x] 更新TestExecutor，集成VL模型到测试执行流程 ✅
  - [x] 实现降级机制（VL失败时自动降级到OCR） ✅
- **优先级5**：LLM集成（5-10天）⭐ **下一步重点**
  - [x] 选择LLM服务（优先使用API方式，快速验证）✅ **已完成（2025-12-21）**
    - [x] 研究国内可用LLM/VL API服务 ✅
    - [x] 整合选型结果到模型部署方案文档（`01-03-模型部署方案.md`） ✅
    - [x] **最终方案确定**：
      - **主方案**：通义千问（LLM: Qwen-Plus + VL: Qwen-VL）✅
      - **备选方案**：DeepSeek-V2（LLM）+ 通义千问VL（VL）✅
    - [ ] 注册并配置API服务（通义千问） ⏳ **下一步**
  - [ ] 设计LLM统一接口（参考VL接口设计）⏳ **下一步**
  - [ ] 实现用例理解功能
  - [ ] 实现操作规划功能
  - [ ] 实现AI自主决策
  - [ ] 测试完整流程（VL + LLM）
- **优先级6**：DEMO验证（3-5天）
  - [ ] 实现完整的测试用例执行流程
  - [ ] 验证VL模型识别准确性
  - [ ] 验证LLM决策能力
  - [ ] 验证纯视觉操作执行
  - [ ] 创建DEMO演示

**路径A：本地部署（后台进行）** ⏳
- 监控ONNX Runtime Issue状态（每天或每2-3天检查Issue #26841）
- 保留现有代码，等待修复后继续

**预计时间**：约3.5-4.5周完成DEMO验证（不阻塞）

**成功标准**：
- **短期（1-2周）**：统一接口设计完成，API客户端实现完成
- **中期（1个月）**：系统集成完成，VL+LLM完整流程可用
- **长期（2-3个月）**：MVP版本核心功能完成

**主要风险与应对**：
- **风险1**：ONNX Runtime问题长时间无法解决 → **应对**：API方式已可用，不影响DEMO验证
- **风险2**：项目进度延误 → **应对**：双路径并行，不阻塞进度

**路径A（本地部署）待实现**（等待ONNX Runtime问题解决后）：
1. ✅ 测试vision_encoder推理 - **已完成（2025-12-15）**
2. ⏳ 实现文本tokenization（使用tokenizer.json，将文本转换为token IDs）⏳ **等待修复**
3. ⏳ 实现文本嵌入（token IDs → 文本嵌入向量，可能需要embed_tokens模型或decoder内置）⏳ **等待修复**
4. ⏳ 合并图像和文本嵌入为inputs_embeds（准备decoder输入）⏳ **等待修复**
5. ⏳ 实现decoder推理（inputs_embeds → logits）⚠️ **当前被ONNX Runtime限制阻塞**
6. ⏳ 实现logits解码为文本（tokenizer decode）⏳ **等待修复**
7. ⏳ 实现JSON解析为ScreenState结构⏳ **等待修复**

