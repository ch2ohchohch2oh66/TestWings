# TestWings 开发路线图

> **本文档是 TestWings 项目的唯一总体规划文档**，记录开发进度、已完成功能和下一步计划。其他文档中的Phase/规划内容已统一到此文档。

## 🎯 终极目标

**TestWings 是一个基于 AI 的 APP 自动化测试框架，能够：**

1. **理解测试用例**：支持自然语言描述，LLM自动理解并生成测试计划
2. **智能屏幕识别**：Vision-Language模型理解屏幕内容（VL为主，OCR为辅）
3. **智能元素定位**：基于VL模型语义匹配，自动定位目标元素，适应UI变化
4. **自动化执行**：纯视觉操作，执行点击、输入、滑动等操作（纯坐标）
5. **智能结果验证**：自动验证测试结果，生成详细测试报告
6. **AI自主决策**：LLM根据用例和屏幕状态自主决策操作

**核心原理**：Vision-Language模型屏幕理解 + LLM自主决策 + 纯视觉操作执行

---

## ✅ 已完成功能

### Phase 0: 项目基础（已完成）
- [x] 项目结构搭建
- [x] Android 开发环境配置
- [x] 基础 UI 界面（Jetpack Compose）
- [x] **屏幕捕获功能** ⭐
  - [x] MediaProjection API 集成
  - [x] 前台服务实现（兼容 HarmonyOS）
  - [x] 截图保存到本地
  - [x] 权限管理
  - [x] MediaProjection 实例复用优化
  - [x] 授权弹窗延迟优化（从 3.5 秒优化到 0-500ms）

### Phase 1: 核心功能（MVP）- 进行中

#### 1.1 基础操作执行 ⭐ **已完成**
**目标**：实现能够操作手机的基础能力（纯坐标操作）

**任务清单：**
- [x] 实现点击操作
  - [x] 根据坐标点击（纯坐标操作）
  - [x] 点击事件分发
- [x] 实现输入操作
  - [x] 文本输入
  - [x] 清空输入框
  - [x] 特殊字符处理（基础支持）
- [x] 实现滑动操作
  - [x] 上下滑动
  - [x] 左右滑动
  - [x] 自定义滑动路径
- [x] 实现系统按键
  - [x] 返回键
  - [x] 主页键
  - [x] 最近任务键
- [x] 测试基本操作流程
  - [x] 创建测试界面
  - [x] 验证点击功能 ✅
  - [x] 验证滑动功能 ✅
  - [x] 验证返回键功能 ✅

**完成时间**：2025-01-08

**测试状态**：✅ 已测试通过
- ✅ 屏幕捕获功能正常
- ✅ 上下左右滑动操作正常
- ✅ 点击中心操作正常
- ✅ 返回键操作正常

**注意**：采用纯视觉AI方案，不依赖AccessibilityService，所有操作基于坐标。

---

## 🚀 下一步计划

### Phase 1: 核心功能（MVP）- 进行中

#### 1.2 OCR 文字识别 ⭐ **已完成**（降级方案）
**目标**：实现OCR识别功能，作为VL模型的降级方案

**完成时间**：2025-12-09

**任务清单：**
- [x] 选择 OCR 库（使用 ML Kit Text Recognition）
- [x] 集成 OCR 库到项目
- [x] 实现截图 OCR 识别功能
- [x] 提取文字内容和位置信息
- [x] 显示识别结果（用于测试和调试）
- [x] 优化识别性能（异步处理）
- [x] 实现自动检测和选择机制（HarmonyOS/Android 自动适配）

**技术选型**：
- **ML Kit Text Recognition**（已采用，作为降级方案）
  - ✅ 轻量级、易集成、免费、支持中文
  - ✅ 已验证在 HarmonyOS 4.2 上可用
  - ⚠️ 识别准确率有限，主要用于降级场景

**定位**：
- ⚠️ **VL为主，OCR为辅**：OCR仅作为VL模型不可用时的降级方案
- ⚠️ 后续将集成Vision-Language模型作为主要能力

**测试状态**：✅ 已测试通过（基础功能）
- ✅ OCR识别功能正常
- ✅ 能够识别屏幕上的文字
- ✅ 能够提取文本块的位置信息（坐标）

---

#### 1.3 Vision-Language模型集成 ⭐ **下一步重点**
**目标**：集成Vision-Language模型，实现屏幕理解能力（主要能力）

**技术选型**：
- **模型**：Qwen2-VL-2B-Instruct（Q4F16量化，推荐）⭐
  - **解码器**：`decoder_model_merged_q4f16.onnx`（约829MB）
  - **视觉编码器**：`vision_encoder_q4f16.onnx`（约1.27GB）
  - **总大小**：约2.1GB
  - **注意**：INT8量化版本使用ConvInteger操作符，ONNX Runtime Android不支持，因此使用Q4F16版本
- **推理框架**：ONNX Runtime for Android (1.20.0)
- **模型位置**：外部存储 `/sdcard/Android/data/com.testwings/files/models/vl/`（避免APK过大）
- **获取方式**：手动下载（从Hugging Face）或使用download_model.py脚本
- **文件识别**：APP自动识别任何.onnx文件，按优先级使用（Q4F16 > 未量化 > INT8）

**详细任务清单**：

**第一阶段：基础准备** ✅ **已完成**
- [x] 添加ONNX Runtime依赖到build.gradle.kts
- [x] 准备Qwen2-VL-2B-Instruct模型文件（INT8量化）和配置文件，放置到外部存储（/sdcard/Android/data/com.testwings/files/models/vl/）
  - [x] 创建ModelDownloader类（支持从Hugging Face下载）
  - [x] 更新VisionLanguageManager（从外部存储加载模型，支持自动识别.onnx文件）
  - [x] 更新文档（Qwen2-VL模型部署操作手册）
- [x] 创建ScreenState数据模型（包含elements、semanticDescription等）
- [x] 创建UIElement数据模型（type、text、bounds、center、confidence等）

**第二阶段：核心实现** ⏳ **进行中**
- [x] 创建VisionLanguageManager类，实现模型加载和初始化框架
- [x] 实现VL模型推理接口框架（understand方法，输入Bitmap，输出ScreenState）
- [x] 实现vision_encoder模型加载和推理逻辑（图像预处理、视觉编码器推理）✅ **已完成**
  - [x] 修复vision_encoder输入格式（patch转换和grid_thw计算）✅ **已完成**
  - [x] 成功加载Q4F16版本的视觉编码器（1270MB）和解码器（829MB）✅ **已验证**
  - [x] 修复模型优先级逻辑（Q4F16优先，INT8自动跳过）✅ **已完成**
- [x] 测试vision_encoder推理（调用runVisionEncoder验证输出）✅ **已完成（2025-12-15）**
  - [x] 修复图像尺寸问题（从960x960改为448x448，根据README.md）
  - [x] 修复temporal_patch_size（从1改为2，模型期望）
  - [x] 修复pixel_values格式（扁平化为[1024, 1176]，模型内部reshape为[1024, 3, 2, 14, 14]）
  - [x] 验证grid_thw格式（[1, 3] = [[1, 32, 32]]）
  - [x] 成功完成推理，输出形状[256, 1536]，耗时~18秒
- [ ] 实现文本tokenization和嵌入（tokenizer.json使用、文本嵌入生成）⭐ **下一步**
- [ ] 实现decoder推理逻辑（合并嵌入、inputs_embeds处理、logits生成）⭐ **下一步**
- [ ] 实现VL模型输出解析（logits解码、JSON解析、转换为ScreenState结构）⭐ **下一步**

**第三阶段：系统集成**
- [ ] 更新ElementLocator，支持VL模型定位（优先使用VL，降级到OCR）
- [ ] 更新SemanticMatcher，支持VL模型语义匹配（替代OCR匹配）
- [ ] 更新TestExecutor，集成VL模型到测试执行流程

**第四阶段：优化和测试**
- [ ] 实现模型加载状态管理（加载中、已加载、加载失败等）
- [ ] 实现模型按需加载和内存管理（避免内存溢出）
- [ ] 添加VL模型可用性检查（模型文件是否存在、是否已加载）
- [ ] 实现错误处理和降级机制（VL失败时自动降级到OCR）
- [ ] 优化推理性能（异步处理、缓存机制、NPU加速支持）
- [ ] 创建VL模型测试用例（验证识别准确性和性能）
- [ ] 集成VL模型到MainActivity（UI显示、状态管理）

**预计时间**：10-15 天

#### 1.4 LLM集成（用例理解和操作规划）
**目标**：集成LLM，实现用例理解和AI自主决策

**任务清单：**
- [ ] 选择LLM模型（Qwen-7B-Chat，INT8量化）
- [ ] 集成LLM到项目
- [ ] 实现用例理解功能
- [ ] 实现操作规划功能
- [ ] AI自主决策能力

**预计时间**：10-15 天

#### 1.5 简单测试用例执行
**目标**：能够执行一个简单的测试用例（基于VL+LLM）

**任务清单：**
- [ ] 设计简单的测试用例格式（自然语言或JSON）
- [ ] LLM理解测试用例
- [ ] VL模型理解屏幕
- [ ] 执行测试步骤（纯视觉操作）
- [ ] 记录执行结果
- [ ] 生成简单报告

**预计时间**：5-7 天

---

#### 1.4 基础报告生成
**目标**：生成测试执行报告

**任务清单：**
- [ ] 设计报告格式
- [ ] 收集执行数据
- [ ] 生成 HTML/JSON 报告
- [ ] 包含截图和日志

**预计时间**：3-5 天

---

## 📋 后续阶段（Phase 2-4）

### Phase 2: AI 增强
- [ ] UI 元素检测（YOLOv8）
- [ ] 本地 LLM 集成（Qwen-1.8B）
- [ ] 用例理解功能
- [ ] 智能元素定位
- [ ] 操作规划

### Phase 3: 完善功能
- [ ] 复杂操作支持（手势、长按等）
- [ ] 结果验证增强
- [ ] 异常处理优化
- [ ] 报告完善

### Phase 4: 优化与扩展
- [ ] 性能优化
- [ ] 插件系统
- [ ] 多平台支持
- [ ] 企业级功能

---

## 🎯 当前里程碑

**当前阶段**：Phase 1 - 核心功能（MVP）

**当前任务**：Vision-Language模型集成 ⭐

**完成度**：约 75%
- ✅ 屏幕捕获（100%）
- ✅ 基础操作执行（100%）
  - ✅ 点击操作（坐标点击、元素点击）
  - ✅ 滑动操作（上下左右、自定义路径）
  - ✅ 输入操作（文本输入、清空）
  - ✅ 系统按键（返回、主页、最近任务）
- ✅ OCR 识别（85%）⭐ 基础功能已完成，准确性待优化
  - ✅ ML Kit Text Recognition 集成
  - ✅ 自动检测和选择机制（HarmonyOS/Android 适配）
  - ✅ OCR结果显示界面
  - ✅ 文本块位置信息提取
  - ⚠️ 识别准确性优化（灰底蓝字场景识别不准确）
- ✅ 用例执行（100%）⭐ 已完成
  - ✅ 测试用例格式设计（JSON）
  - ✅ 测试用例解析和执行
  - ✅ 截图和OCR集成
  - ✅ 验证功能
- ⏳ Vision-Language模型集成（50%）⭐ 下一步重点
  - ✅ 基础准备（依赖、模型文件、数据结构）- 已完成
  - ⏳ 核心实现（模型加载完成，vision_encoder推理已验证成功）- 进行中
    - ✅ 模型加载和初始化框架 - 已完成
    - ✅ vision_encoder输入格式修复（patch转换和grid_thw计算）- 已完成
    - ✅ vision_encoder推理测试 - **已完成（2025-12-15）** ⭐
      - ✅ 图像尺寸修复（448x448）
      - ✅ temporal_patch_size修复（2）
      - ✅ pixel_values格式验证（[1024, 1176]）
      - ✅ 推理成功，输出[256, 1536]，耗时~18秒
    - ⏳ 文本tokenization和嵌入 - 待实现
    - ⏳ decoder推理和输出解析 - 待实现
  - ⏳ 系统集成（ElementLocator、SemanticMatcher、TestExecutor）- 待开始
  - ⏳ 优化和测试（性能优化、错误处理、测试验证）- 待开始
- ⏳ OCR准确性优化（0%）- 已确定VL为主，OCR为辅，暂缓
- ⏳ 报告生成（0%）

---

## 📝 开发原则

1. **先做能用的，再做完美的**
   - MVP 版本优先实现核心功能
   - 后续逐步优化和完善

2. **循序渐进**
   - 每个功能独立实现和测试
   - 确保每个功能稳定后再继续

3. **本地优先**
   - 优先使用本地模型和本地处理
   - 降低环境复杂度

4. **文档同步**
   - 代码和文档同步更新
   - 记录开发过程中的问题和解决方案

---

## 🔄 更新记录

- **2025-01-08**：完成屏幕捕获功能，开始规划下一步
- **2025-01-08**：创建开发路线图文档
- **2025-01-08**：完成基础操作执行功能
  - ✅ 实现 AccessibilityService
  - ✅ 实现点击、滑动、输入、系统按键等操作
  - ✅ 创建操作测试界面和反馈机制
  - ✅ 所有基础操作测试通过
- **2025-01-08**：更新进度，确定下一步为 OCR 文字识别
- **2025-01-08**：优化屏幕捕获授权弹窗延迟
  - ✅ 优化授权弹窗显示延迟（从 3.5 秒优化到 0-500ms）
  - ✅ 添加智能服务状态检查（服务已运行时立即显示弹窗）
  - ✅ 减少固定延迟时间（前台服务启动通常很快）
  - ✅ 优化授权后捕获延迟（从 1.5 秒优化到 500ms）
- **2025-12-09**：完成 OCR 文字识别功能
  - ✅ 集成 ML Kit Text Recognition
  - ✅ 实现 OCR 识别功能（截图后自动识别）
  - ✅ 创建 OCR 结果显示界面（文本块详情、位置信息）
  - ✅ 实现自动检测和选择机制（HarmonyOS/Android 自动适配）
  - ✅ 验证 ML Kit 在 HarmonyOS 4.2 上可用（即使没有 Google Play Services）
  - ✅ 创建 OCR 功能验证报告文档
- **2025-01-11**：完成简单测试用例执行功能
  - ✅ 设计测试用例格式（JSON）
  - ✅ 实现测试用例解析器
  - ✅ 实现测试执行引擎（TestExecutor）
  - ✅ 实现元素定位器（ElementLocator）
  - ✅ 集成截图和OCR到测试执行流程
  - ✅ 修复截图重复问题（一次截图操作只产生一张截图）
  - ✅ 创建测试用例管理界面
- **2025-01-11**：发现 OCR 识别准确性问题
  - ⚠️ 灰底蓝字场景识别不准确（如"捕获屏幕"识别成"鋪获屏幕"或"埔获屏幕"）
  - 📋 制定优化方案：图像预处理增强、后处理纠错、多引擎融合
- **2025-01-11**：确定技术路线 - VL为主，OCR为辅
  - ✅ 从工程实践角度评估：VL模型是核心能力，必需集成
  - ✅ PaddleOCR可作为更好的降级方案（替代ML Kit），但不能替代VL
  - ✅ 制定VL模型集成详细TODO清单（18项任务，分4个阶段）
  - 📋 下一步：开始VL模型集成，预计10-15天完成
- **2025-12-14**：完成VL模型集成第一阶段（基础准备）
  - ✅ 模型选型确定：Qwen2-VL-2B-Instruct（从Qwen-VL-Chat改为更小的2B版本）
  - ✅ 模型文件部署方案：外部存储，避免APK过大
  - ✅ 创建VisionLanguageManager框架（模型检查、加载状态管理）
  - ✅ 实现模型文件自动识别（通过.onnx后缀，按优先级查找）
  - ✅ 创建ScreenState和UIElement数据模型
  - ✅ 完成模型部署文档（Qwen2-VL模型部署操作手册）
  - 📋 下一步：实现ONNX Runtime模型推理逻辑（图像预处理、模型加载、推理执行）
- **2025-12-14**：完成vision_encoder加载和基础推理实现
  - ✅ 添加vision_encoder模型文件查找和加载逻辑（支持INT8和Q4F16量化版本）
  - ✅ 实现图像预处理（调整到960x960，保持宽高比，归一化到[0,1]）
  - ✅ 实现vision_encoder推理框架（图像 → 图像嵌入向量）
  - ✅ 修复OutOfMemoryError（使用文件路径加载模型，避免一次性读取到内存）
  - ✅ 更新模型部署文档（添加vision_encoder下载说明）
  - ✅ 发现INT8量化模型不兼容问题（ConvInteger操作符不支持），改用Q4F16版本
  - ✅ 成功加载Q4F16版本的视觉编码器（1270MB）和解码器（829MB）✅ **已验证**
  - ✅ 确认视觉编码器输入格式：`pixel_values`（扁平化patch数据）和 `grid_thw`（网格信息）
  - ✅ 修复vision_encoder输入格式（实现patch转换和grid_thw计算，使用ceil确保正确）✅ **已完成**
  - ✅ 修复模型优先级逻辑（Q4F16优先，INT8版本自动跳过并提示错误）✅ **已完成**
  - ✅ 修复LongBuffer导入和grid_thw张量创建（使用LongBuffer.wrap包装LongArray）✅ **已完成**
- **2025-12-15**：完成vision_encoder推理测试 ✅ **测试成功**
  - ✅ 修复图像尺寸问题（从960x960改为448x448，根据README.md示例）
  - ✅ 修复temporal_patch_size（从1改为2，模型期望reshape为[-1, 3, 2, 14, 14]）
  - ✅ 修复pixel_values格式（扁平化为[num_patches, 1176]，模型内部reshape为[num_patches, 3, 2, 14, 14]）
  - ✅ 验证grid_thw格式（[1, 3] = [[grid_t, grid_h, grid_w]] = [[1, 32, 32]]）
  - ✅ 成功完成推理测试：输出形状[256, 1536]，推理耗时~18秒
  - ✅ 更新模型结构研究文档（添加已验证的输入输出格式和测试结果）
  - 📋 下一步：实现文本tokenization、decoder推理和输出解析

---

**下一步行动**：实现完整的VL模型推理流程。已完成vision_encoder推理测试 ✅。下一步需要：
1. ✅ **测试vision_encoder推理** - **已完成（2025-12-15）**
   - ✅ 输出形状验证：[256, 1536]（符合预期）
   - ✅ 推理耗时：~18秒（在预期范围内）
2. 实现文本tokenization（使用tokenizer.json，将文本转换为token IDs）⭐ **下一步优先**
3. 实现文本嵌入（token IDs → 文本嵌入向量，可能需要embed_tokens模型或decoder内置）
4. 合并图像和文本嵌入为inputs_embeds（准备decoder输入）
5. 实现decoder推理（inputs_embeds → logits）
6. 实现logits解码为文本（tokenizer decode）
7. 实现JSON解析为ScreenState结构

