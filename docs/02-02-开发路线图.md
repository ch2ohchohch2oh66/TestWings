# TestWings 开发路线图

> **本文档是 TestWings 项目的唯一总体规划文档**，记录开发进度、已完成功能和下一步计划。其他文档中的Phase/规划内容已统一到此文档。

## 🎯 终极目标

**TestWings 是一个基于 AI 的 APP 自动化测试框架，能够：**

1. **理解测试用例**：支持自然语言描述，LLM自动理解并生成测试计划
2. **智能屏幕识别**：Vision-Language模型理解屏幕内容（VL为主，OCR为辅）
3. **智能元素定位**：基于VL模型语义匹配，自动定位目标元素，适应UI变化
4. **自动化执行**：纯视觉操作，执行点击、输入、滑动等操作（纯坐标）
5. **智能结果验证**：自动验证测试结果，生成详细测试报告
6. **AI自主决策**：LLM根据用例和屏幕状态自主决策操作

**核心原理**：Vision-Language模型屏幕理解 + LLM自主决策 + 纯视觉操作执行

---

## ✅ 已完成功能

### Phase 0: 项目基础（已完成）
- [x] 项目结构搭建
- [x] Android 开发环境配置
- [x] 基础 UI 界面（Jetpack Compose）
- [x] **屏幕捕获功能** ⭐
  - [x] MediaProjection API 集成
  - [x] 前台服务实现（兼容 HarmonyOS）
  - [x] 截图保存到本地
  - [x] 权限管理
  - [x] MediaProjection 实例复用优化
  - [x] 授权弹窗延迟优化（从 3.5 秒优化到 0-500ms）

### Phase 1: 核心功能（MVP）- 进行中

#### 1.1 基础操作执行 ⭐ **已完成**
**目标**：实现能够操作手机的基础能力（纯坐标操作）

**任务清单：**
- [x] 实现点击操作
  - [x] 根据坐标点击（纯坐标操作）
  - [x] 点击事件分发
- [x] 实现输入操作
  - [x] 文本输入
  - [x] 清空输入框
  - [x] 特殊字符处理（基础支持）
- [x] 实现滑动操作
  - [x] 上下滑动
  - [x] 左右滑动
  - [x] 自定义滑动路径
- [x] 实现系统按键
  - [x] 返回键
  - [x] 主页键
  - [x] 最近任务键
- [x] 测试基本操作流程
  - [x] 创建测试界面
  - [x] 验证点击功能 ✅
  - [x] 验证滑动功能 ✅
  - [x] 验证返回键功能 ✅

**完成时间**：2025-01-08

**测试状态**：✅ 已测试通过
- ✅ 屏幕捕获功能正常
- ✅ 上下左右滑动操作正常
- ✅ 点击中心操作正常
- ✅ 返回键操作正常

**注意**：采用纯视觉AI方案，不依赖AccessibilityService，所有操作基于坐标。

---

## 🚀 下一步计划

### Phase 1: 核心功能（MVP）- 进行中

#### 1.2 OCR 文字识别 ⭐ **已完成**（降级方案）
**目标**：实现OCR识别功能，作为VL模型的降级方案

**完成时间**：2025-12-09

**任务清单：**
- [x] 选择 OCR 库（使用 ML Kit Text Recognition）
- [x] 集成 OCR 库到项目
- [x] 实现截图 OCR 识别功能
- [x] 提取文字内容和位置信息
- [x] 显示识别结果（用于测试和调试）
- [x] 优化识别性能（异步处理）
- [x] 实现自动检测和选择机制（HarmonyOS/Android 自动适配）

**技术选型**：
- **ML Kit Text Recognition**（已采用，作为降级方案）
  - ✅ 轻量级、易集成、免费、支持中文
  - ✅ 已验证在 HarmonyOS 4.2 上可用
  - ⚠️ 识别准确率有限，主要用于降级场景

**定位**：
- ⚠️ **VL为主，OCR为辅**：OCR仅作为VL模型不可用时的降级方案
- ⚠️ 后续将集成Vision-Language模型作为主要能力

**测试状态**：✅ 已测试通过（基础功能）
- ✅ OCR识别功能正常
- ✅ 能够识别屏幕上的文字
- ✅ 能够提取文本块的位置信息（坐标）

---

#### 1.3 Vision-Language模型集成 ⭐ **下一步重点**
**目标**：集成Vision-Language模型，实现屏幕理解能力（主要能力）

**任务清单：**
- [ ] 选择VL模型（Qwen-VL-Chat，INT8量化）
- [ ] 集成VL模型到项目
- [ ] 实现屏幕理解功能
- [ ] 识别所有UI元素（文字、按钮、输入框等）
- [ ] 提取元素坐标和语义描述
- [ ] 优化模型推理性能

**预计时间**：10-15 天

#### 1.4 LLM集成（用例理解和操作规划）
**目标**：集成LLM，实现用例理解和AI自主决策

**任务清单：**
- [ ] 选择LLM模型（Qwen-7B-Chat，INT8量化）
- [ ] 集成LLM到项目
- [ ] 实现用例理解功能
- [ ] 实现操作规划功能
- [ ] AI自主决策能力

**预计时间**：10-15 天

#### 1.5 简单测试用例执行
**目标**：能够执行一个简单的测试用例（基于VL+LLM）

**任务清单：**
- [ ] 设计简单的测试用例格式（自然语言或JSON）
- [ ] LLM理解测试用例
- [ ] VL模型理解屏幕
- [ ] 执行测试步骤（纯视觉操作）
- [ ] 记录执行结果
- [ ] 生成简单报告

**预计时间**：5-7 天

---

#### 1.4 基础报告生成
**目标**：生成测试执行报告

**任务清单：**
- [ ] 设计报告格式
- [ ] 收集执行数据
- [ ] 生成 HTML/JSON 报告
- [ ] 包含截图和日志

**预计时间**：3-5 天

---

## 📋 后续阶段（Phase 2-4）

### Phase 2: AI 增强
- [ ] UI 元素检测（YOLOv8）
- [ ] 本地 LLM 集成（Qwen-1.8B）
- [ ] 用例理解功能
- [ ] 智能元素定位
- [ ] 操作规划

### Phase 3: 完善功能
- [ ] 复杂操作支持（手势、长按等）
- [ ] 结果验证增强
- [ ] 异常处理优化
- [ ] 报告完善

### Phase 4: 优化与扩展
- [ ] 性能优化
- [ ] 插件系统
- [ ] 多平台支持
- [ ] 企业级功能

---

## 🎯 当前里程碑

**当前阶段**：Phase 1 - 核心功能（MVP）

**当前任务**：优化 OCR 识别准确性

**完成度**：约 75%
- ✅ 屏幕捕获（100%）
- ✅ 基础操作执行（100%）
  - ✅ 点击操作（坐标点击、元素点击）
  - ✅ 滑动操作（上下左右、自定义路径）
  - ✅ 输入操作（文本输入、清空）
  - ✅ 系统按键（返回、主页、最近任务）
- ✅ OCR 识别（85%）⭐ 基础功能已完成，准确性待优化
  - ✅ ML Kit Text Recognition 集成
  - ✅ 自动检测和选择机制（HarmonyOS/Android 适配）
  - ✅ OCR结果显示界面
  - ✅ 文本块位置信息提取
  - ⚠️ 识别准确性优化（灰底蓝字场景识别不准确）
- ✅ 用例执行（100%）⭐ 已完成
  - ✅ 测试用例格式设计（JSON）
  - ✅ 测试用例解析和执行
  - ✅ 截图和OCR集成
  - ✅ 验证功能
- ⏳ OCR准确性优化（0%）⭐ 下一步
- ⏳ 报告生成（0%）

---

## 📝 开发原则

1. **先做能用的，再做完美的**
   - MVP 版本优先实现核心功能
   - 后续逐步优化和完善

2. **循序渐进**
   - 每个功能独立实现和测试
   - 确保每个功能稳定后再继续

3. **本地优先**
   - 优先使用本地模型和本地处理
   - 降低环境复杂度

4. **文档同步**
   - 代码和文档同步更新
   - 记录开发过程中的问题和解决方案

---

## 🔄 更新记录

- **2025-01-08**：完成屏幕捕获功能，开始规划下一步
- **2025-01-08**：创建开发路线图文档
- **2025-01-08**：完成基础操作执行功能
  - ✅ 实现 AccessibilityService
  - ✅ 实现点击、滑动、输入、系统按键等操作
  - ✅ 创建操作测试界面和反馈机制
  - ✅ 所有基础操作测试通过
- **2025-01-08**：更新进度，确定下一步为 OCR 文字识别
- **2025-01-08**：优化屏幕捕获授权弹窗延迟
  - ✅ 优化授权弹窗显示延迟（从 3.5 秒优化到 0-500ms）
  - ✅ 添加智能服务状态检查（服务已运行时立即显示弹窗）
  - ✅ 减少固定延迟时间（前台服务启动通常很快）
  - ✅ 优化授权后捕获延迟（从 1.5 秒优化到 500ms）
- **2025-12-09**：完成 OCR 文字识别功能
  - ✅ 集成 ML Kit Text Recognition
  - ✅ 实现 OCR 识别功能（截图后自动识别）
  - ✅ 创建 OCR 结果显示界面（文本块详情、位置信息）
  - ✅ 实现自动检测和选择机制（HarmonyOS/Android 自动适配）
  - ✅ 验证 ML Kit 在 HarmonyOS 4.2 上可用（即使没有 Google Play Services）
  - ✅ 创建 OCR 功能验证报告文档
- **2025-01-11**：完成简单测试用例执行功能
  - ✅ 设计测试用例格式（JSON）
  - ✅ 实现测试用例解析器
  - ✅ 实现测试执行引擎（TestExecutor）
  - ✅ 实现元素定位器（ElementLocator）
  - ✅ 集成截图和OCR到测试执行流程
  - ✅ 修复截图重复问题（一次截图操作只产生一张截图）
  - ✅ 创建测试用例管理界面
- **2025-01-11**：发现 OCR 识别准确性问题
  - ⚠️ 灰底蓝字场景识别不准确（如"捕获屏幕"识别成"鋪获屏幕"或"埔获屏幕"）
  - 📋 制定优化方案：图像预处理增强、后处理纠错、多引擎融合

---

**下一步行动**：优化 OCR 识别准确性，重点解决灰底蓝字场景识别不准确的问题。优先实施方案一（图像预处理增强），预计提升准确率 20-30%。

