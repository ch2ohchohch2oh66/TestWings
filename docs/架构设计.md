# TestWings 架构设计文档

## 一、核心设计理念

### 1.1 设计目标

**TestWings 是一个完全基于视觉AI的自动化测试框架**，核心特点：

1. **AI视觉理解**：
   - **LLM理解自然语言用例**：理解测试用例的意图和执行状态，制定操作策略
   - **VL/大模型识别屏幕元素**：识别屏幕上的文字、按钮、输入框等所有可操作元素
   - **AI自主决策**：根据用例和屏幕状态，自主规划下一步操作
2. **类人操作**：像人一样观察屏幕、理解内容、做出决策、执行操作
3. **操作执行策略**：
   - **最优方式**：Accessibility Service（如果可用）
     - **准确性**：100%（直接操作控件节点）
     - **高效性**：<0.01秒（不需要VL识别）
     - **稳定性**：最高（不依赖坐标，直接操作控件）
     - **工程实践评估**：这是普通应用唯一可行的自动化操作方案，且用户只需一次授权
   - **降级方式**：纯坐标操作（Accessibility Service不可用时）
     - **准确性**：90-95%（依赖VL识别准确性）
     - **高效性**：2-5秒（需要VL模型推理）
     - **稳定性**：中等（依赖坐标准确性）
     - **适用场景**：Accessibility Service不可用时的降级方案
4. **实时响应**：实时获取屏幕信息，实时决策，实时执行

### 1.2 核心能力

- **视觉理解**：理解屏幕上的所有元素（文字、按钮、图标、布局等）
- **语义理解**：理解用例意图和屏幕内容的语义
- **策略规划**：根据用例和屏幕状态，制定操作步骤
- **自主决策**：像人一样理解任何屏幕情况，自主决策处理方式（不需要穷举场景）
- **操作执行**：将决策转化为具体的操作（点击、滑动、输入等）
- **预处理机制**（必需的性能优化，从工程实践角度）：
  - **性能提升**：将响应时间从2-5秒降低到<0.1秒，提升执行效率10-50倍
  - **准确性提升**：预处理使用完整页面截图，识别更全面，准确率更高
  - **成本**：预处理只需一次，后续直接使用结果，成本低
  - **工程实践结论**：这是必需的性能优化，不是可选功能

### 1.3 典型场景

- **打开指定APP**：AI滑动查找，找到后点击
- **处理广告**：AI识别并自动关闭
- **处理更新弹窗**：AI根据用例要求决策
- **处理权限弹窗**：AI智能识别并决策
- **处理意外弹窗**：AI根据场景智能决策

**注意**：这些场景由AI自主处理，不硬编码逻辑。

### 1.4 技术原理

**纯视觉AI自动化测试的核心原理**：

1. **屏幕内容识别**：
   - 屏幕截图 → Vision-Language模型理解 → 识别所有元素（文字、按钮、输入框等）
   - VL为主，OCR为辅（降级方案）

2. **任务理解与规划**：
   - 用户指令（自然语言） → LLM理解意图 → 任务分解 → 操作序列生成

3. **执行控制**：
   - 操作指令 → VL模型元素定位（纯视觉） → 操作执行（纯坐标） → 结果验证 → 下一步决策

4. **数据流转**：
   - **最优方案**：完全本地部署
     - 所有AI模型在手机本地运行
     - **优势**：无网络延迟、隐私安全、离线可用、无API费用
     - **工程实践评估**：这是最优方案，除非有特殊需求
   - **备选方案**：混合部署（不推荐，除非有特殊需求）
     - 轻量级模型本地，大模型云端
     - **劣势**：增加网络延迟、隐私风险、API费用、依赖网络
     - **适用场景**：需要最新能力，且可以接受上述劣势

---

## 二、整体架构

### 2.1 系统分层架构

```
┌─────────────────────────────────────────────────────────────┐
│                      用户接口层                               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  Web管理端   │  │  移动端APP   │  │  CLI命令行   │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      测试用例管理层                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  用例解析    │  │  用例存储    │  │  用例执行    │      │
│  │  (Parser)    │  │  (Database)  │  │  (Executor)  │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      AI核心引擎层                            │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  用例理解    │  │  屏幕理解    │  │  操作规划    │      │
│  │  (LLM)       │  │  (Vision+OCR)│  │  (LLM)       │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      设备控制层                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  屏幕捕获    │  │  元素定位    │  │  操作执行    │      │
│  │  (Capture)   │  │  (Locator)   │  │  (Executor)  │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      结果分析层                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  结果验证    │  │  报告生成    │  │  问题分析    │      │
│  │  (Verifier)  │  │  (Reporter)  │  │  (Analyzer)  │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 核心模块说明

#### 2.2.1 用户接口层
- **Web管理端**：提供测试用例管理、执行监控、报告查看等功能
- **移动端APP**：作为测试执行代理，安装在测试设备上
- **CLI命令行**：提供命令行工具，支持CI/CD集成

#### 2.2.2 测试用例管理层
- **用例解析**：支持多种格式（自然语言、JSON、YAML、Python脚本）
- **用例存储**：用例数据库，支持版本管理
- **用例执行**：测试执行引擎，支持并行执行

#### 2.2.3 AI核心引擎层
- **用例理解**：使用LLM理解测试用例意图（自然语言）
- **屏幕理解**：使用Vision-Language模型理解屏幕内容（VL为主，OCR为辅）
- **操作规划**：LLM像人一样理解屏幕情况，自主规划操作（通用理解，不穷举场景）
  - **核心能力**：LLM具备通用理解能力，能够理解任何屏幕情况并做出决策
  - **实时决策**：根据当前屏幕的实际内容，像人一样分析并决策
  - **灵活应对**：能够处理从未见过的场景，适应各种变化

#### 2.2.4 设备控制层
- **屏幕捕获**：实时获取设备屏幕截图（MediaProjection API）
- **元素定位**：
  - **主要方式**：VL综合定位
    - 准确性：90-95%，高效性：2-5秒（预处理后<0.1秒）
    - VL模型一次识别，同时输出控件信息和文本信息
    - 综合匹配：控件类型 + 文本内容（如"按钮" + "登录" → "登录按钮"）
    - **优势**：准确率高，容错性强，覆盖率100%
  - **增强方式**：VL识别 + Accessibility Service验证（如果可用）
    - VL识别元素后，如果Accessibility Service可用，通过TEXT或contentDescription在Accessibility树中查找匹配节点
    - 找到匹配节点后，使用Accessibility Service操作（更稳定）
    - **优势**：结合VL的识别能力和Accessibility Service的稳定性
  - **快速路径**：RESOURCE_ID定位（如果已知Resource ID）
    - 准确性：100%，高效性：<0.01秒
    - **Resource ID获取方式**：
      1. **预处理阶段**：VL识别元素后，通过位置坐标在Accessibility树中查找节点，获取Resource ID（如果存在）
      2. **实时获取**：VL识别元素后，通过文本/类型在Accessibility树中查找匹配节点，获取Resource ID（如果存在）
      3. **历史记录**：之前执行时获取的Resource ID，存储到历史记录中
    - 如果Accessibility Service可用，且已知Resource ID（从预处理结果或历史记录）
    - 直接通过Resource ID查找元素
    - **限制**：需要Accessibility Service，且需要预先知道Resource ID（覆盖率约30-50%，只有部分元素有Resource ID）
  - **降级方案**：TEXT定位（仅文本匹配）
    - 准确性：80-90%，高效性：2-5秒
    - 当VL综合定位失败时，仅使用文本内容匹配
- **操作执行**：执行点击、滑动、输入等操作
  - **最优方式**：Accessibility Service（如果可用）
    - **准确性**：⭐⭐⭐⭐⭐（100%，直接操作控件节点）
    - **高效性**：⭐⭐⭐⭐⭐（<0.01秒，不需要VL识别）
    - **稳定性**：⭐⭐⭐⭐⭐（最稳定，不依赖坐标）
    - **权限要求**：用户只需一次授权开启Accessibility Service，之后可以完全自动化
    - **操作方式**：
      - 直接操作节点：`node.performAction(ACTION_CLICK)`（最优，最稳定）
      - 获取坐标后点击：`getBoundsInScreen()` + `clickAt()`（备选，可获取控件边界信息）
    - **适用场景**：所有操作场景，这是工程实践最优解
  - **降级方式**：纯坐标操作（Accessibility Service不可用时）
    - **准确性**：⭐⭐⭐⭐（90-95%，依赖VL识别准确性）
    - **高效性**：⭐⭐⭐（2-5秒，需要VL模型推理）
    - **稳定性**：⭐⭐⭐（依赖坐标准确性）
    - 基于VL识别的控件中心坐标
    - 点击控件中心，不是文本中心
    - **适用场景**：Accessibility Service不可用时的降级方案

#### 2.2.5 结果分析层
- **结果验证**：验证测试步骤执行结果
- **报告生成**：生成测试报告（HTML、JSON、PDF）
- **问题分析**：分析失败原因，提供修复建议

---

## 三、核心流程设计

### 3.1 测试执行主流程

```python
# 伪代码示例
class TestExecutor:
    def execute_test_case(self, test_case):
        # 1. 理解测试用例
        test_plan = self.ai_engine.understand_test_case(test_case)
        
        # 2. 执行测试步骤
        for step in test_plan.steps:
            # 2.1 捕获屏幕
            screenshot = self.device_controller.capture_screen()
            
            # 2.2 理解屏幕内容
            screen_state = self.ai_engine.understand_screen(screenshot)
            
            # 2.3 匹配目标元素
            target_element = self.element_locator.find_element(
                step.target, screen_state
            )
            
            # 2.4 执行操作
            result = self.device_controller.execute_action(
                step.action, target_element, step.params
            )
            
            # 2.5 验证结果
            verification_result = self.result_verifier.verify(
                step.expected, screenshot, result
            )
            
            # 2.6 记录结果
            self.result_recorder.record_step_result(
                step, result, verification_result
            )
            
            # 2.7 处理异常
            if not verification_result.success:
                self.exception_handler.handle(verification_result)
        
        # 3. 生成报告
        report = self.report_generator.generate(test_case, results)
        return report
```

### 3.2 AI理解流程

```python
# 用例理解流程
class TestCaseUnderstanding:
    def understand_test_case(self, test_case):
        # 1. 解析用例格式
        parsed_case = self.parser.parse(test_case)
        
        # 2. 使用LLM理解意图
        prompt = self.build_understanding_prompt(parsed_case)
        understanding = self.llm.generate(prompt)
        
        # 3. 生成测试计划
        test_plan = self.plan_generator.generate(understanding)
        
        return test_plan

# 屏幕理解流程（纯视觉AI方案）
class ScreenUnderstanding:
    def understand_screen(self, screenshot):
        # 1. 优先使用Vision-Language模型理解屏幕
        # VL模型同时识别文字、UI元素、布局结构
        screen_understanding = self.vl_model.understand(screenshot)
        
        # 2. OCR作为降级方案（可选）
        # 仅在VL模型不可用时使用
        if not screen_understanding:
            text_elements = self.ocr_engine.recognize(screenshot)
            screen_understanding = self.build_from_ocr(text_elements)
        
        # 3. LLM根据屏幕理解结果进行决策
        decision = self.llm.make_decision(
            screen_understanding=screen_understanding,
            test_case_context=self.test_case_context
        )
        
        return ScreenState(
            elements=screen_understanding.elements,  # 所有UI元素（文字、按钮、输入框等）
            semantic_description=screen_understanding.description,
            decision=decision  # LLM的决策结果
        )
```

---

## 四、技术栈选型

### 4.1 移动端（Android/iOS）

#### Android
- **屏幕捕获**：MediaProjection API（必需，唯一可行的屏幕捕获方案）
- **屏幕理解**：Vision-Language模型（Qwen-VL等，纯视觉，必需）
- **操作执行**：
  - **最优方式**：Accessibility Service（普通应用唯一可行的自动化操作方案）
    - **准确性**：100%，**高效性**：<0.01秒，**稳定性**：最高
    - 用户只需一次授权开启，之后可以完全自动化
    - 支持直接操作节点（performAction，最优）或坐标操作（getBoundsInScreen + clickAt，备选）
    - **注意**：Instrumentation API和UiAutomation API需要系统权限，普通应用无法使用
  - **降级方式**：纯坐标操作（Accessibility Service不可用时）
    - **准确性**：90-95%，**高效性**：2-5秒（需要VL识别），**稳定性**：中等
    - 基于VL模型识别的控件中心坐标
- **OCR**：ML Kit Text Recognition（降级方案，仅在VL不可用时使用）
- **图像处理**：OpenCV for Android（可选，用于图像预处理和特征提取）

#### iOS
- **屏幕捕获**：ScreenCapture API
- **UI访问**：XCUITest / Accessibility API
- **操作执行**：XCUITest / 私有API（需越狱）
- **OCR**：Vision Framework / PaddleOCR
- **图像处理**：Core Image / OpenCV

### 4.2 AI模型选型

#### 大语言模型（LLM）- 本地部署（最优方案）

**工程实践评估**：
- **准确性**：⭐⭐⭐⭐（Qwen-7B：高，Qwen-1.8B：中等）
- **高效性**：⭐⭐⭐（2-5秒，本地推理）
- **资源占用**：⭐⭐⭐（Qwen-7B：4-5GB，Qwen-1.8B：1-2GB）
- **隐私安全**：⭐⭐⭐⭐⭐（完全本地，不上传数据）
- **成本**：⭐⭐⭐⭐⭐（一次部署，无API费用）

**推荐模型**：
- **Qwen-7B-Chat（INT8 量化）**：准确性高，资源占用中等（4-5GB），推荐用于生产环境
- **Qwen-1.8B-Chat（INT8 量化）**：资源占用低（1-2GB），准确性中等，适合资源受限设备

**推理框架**：
- **最优方案**：ONNX Runtime（性能好，跨平台支持好）
- **备选方案**：TensorFlow Lite（如果ONNX Runtime不可用）
**用途**：用例理解、操作规划、屏幕语义理解（共享同一模型，降低成本）

#### 视觉模型 - 本地部署（最优方案）

**Vision-Language模型**：Qwen-VL-Chat（INT8 量化）⭐
- **准确性**：⭐⭐⭐⭐（90-95%，同时识别控件和文本）
- **高效性**：⭐⭐⭐（2-5秒，本地推理）
- **资源占用**：⭐⭐⭐（4-5GB）
- **隐私安全**：⭐⭐⭐⭐⭐（完全本地）
- **必需性**：核心能力，必需部署

**OCR**：ML Kit Text Recognition（降级方案）
- **准确性**：⭐⭐⭐（80-90%，仅文字）
- **高效性**：⭐⭐⭐⭐（<1秒）
- **资源占用**：⭐⭐⭐⭐⭐（~100MB）
- **适用场景**：VL不可用时的降级方案

**目标检测**：YOLOv8 Nano（备选方案，不推荐）
- **工程实践评估**：VL模型已经能够同时识别控件和文本，YOLOv8是冗余的
- **适用场景**：仅在VL模型完全不可用时的极端降级方案
- **结论**：从工程实践角度，不推荐使用，VL模型已经足够

#### 企业级备选方案（不推荐，除非有特殊需求）

**云端方案**（增加网络延迟和隐私风险）：
- **云端 LLM**：GPT-4、Claude、Qwen-7B/14B（自建服务）
- **云端视觉理解**：GPT-4V、Claude Vision、Qwen-VL
- **适用场景**：需要最新能力，且可以接受网络延迟和隐私风险

### 4.3 数据存储（完全本地）

- **数据库**：SQLite（用例存储、执行结果）
- **文件存储**：本地文件系统（截图、日志、报告）
- **缓存**：内存缓存（模型推理结果、UI 元素识别结果）

### 4.4 企业级后端服务（可选）

**仅在企业级混合部署场景需要：**
- **Web框架**：FastAPI / Flask
- **数据库**：PostgreSQL（用例存储）、Redis（缓存）
- **消息队列**：RabbitMQ / Kafka（异步任务）
- **任务调度**：Celery
- **API网关**：Nginx
- **LLM服务**：vLLM、TensorRT-LLM

### 4.5 移动端APP

- **开发框架**：Android 原生（Kotlin/Java）
- **UI框架**：Jetpack Compose 或传统 View 系统
- **数据库**：Room（SQLite 封装）
- **异步处理**：Kotlin Coroutines

---

## 五、数据模型设计

### 5.1 测试用例模型

```python
class TestCase:
    id: str
    name: str
    description: str
    format: str  # 'natural_language', 'json', 'yaml', 'python'
    content: str  # 原始用例内容
    parsed_content: dict  # 解析后的结构化内容
    app_package: str  # 目标应用包名
    created_at: datetime
    updated_at: datetime
    version: int

class TestStep:
    step_id: int
    action: str  # 'click', 'input', 'swipe', 'verify', 'wait'
    target: str  # 目标元素描述
    params: dict  # 操作参数
    expected: dict  # 预期结果
    timeout: int  # 超时时间（秒）
```

### 5.2 屏幕状态模型

```python
class ScreenState:
    screenshot_path: str
    timestamp: datetime
    elements: List[UIElement]  # VL模型识别的所有元素（文字、按钮、输入框等）
    semantic_description: str  # VL模型生成的语义描述
    decision: Decision  # LLM的决策结果

class TextElement:
    text: str
    bbox: BoundingBox  # 边界框
    confidence: float

class UIElement:
    type: str  # 'button', 'input', 'image', 'text'
    bbox: BoundingBox
    attributes: dict  # 属性（颜色、大小等）
    confidence: float
```

### 5.3 执行结果模型

```python
class TestExecution:
    id: str
    test_case_id: str
    device_id: str
    status: str  # 'running', 'passed', 'failed', 'error'
    start_time: datetime
    end_time: datetime
    steps: List[StepExecution]
    screenshots: List[str]
    report_path: str

class StepExecution:
    step_id: int
    action: str
    target: str
    status: str  # 'success', 'failed', 'skipped'
    execution_time: float
    screenshot_before: str
    screenshot_after: str
    error_message: str
    verification_result: VerificationResult
```

---

## 六、关键算法设计

### 6.1 元素定位算法

```python
class ElementLocator:
    def find_element(self, target_description, screen_state):
        """
        多策略元素定位
        """
        # 策略1：VL模型语义匹配（优先）
        # VL模型已经识别了所有元素及其语义描述
        candidates = self.vl_semantic_match(target_description, screen_state)
        if candidates:
            return self.select_best_candidate(candidates)
        
        # 策略2：LLM辅助匹配
        # 使用LLM理解目标描述，匹配VL识别的元素
        candidates = self.llm_assisted_match(target_description, screen_state)
        if candidates:
            return self.select_best_candidate(candidates)
        
        # 策略3：OCR降级匹配（可选）
        # 仅在VL模型不可用时使用
        if screen_state.ocr_fallback:
            candidates = self.ocr_text_match(target_description, screen_state)
            if candidates:
                return self.select_best_candidate(candidates)
        
        raise ElementNotFoundError(f"无法找到元素: {target_description}")
    
    def vl_semantic_match(self, target, screen_state):
        """
        使用Vision-Language模型进行语义匹配
        VL模型已经识别了所有元素及其语义描述
        """
        # VL模型输出的元素列表包含语义描述
        for element in screen_state.elements:
            if self.is_semantic_match(target, element.semantic_description):
                return [element]
        return []
    
    def llm_assisted_match(self, target, screen_state):
        """
        使用LLM辅助匹配
        """
        prompt = f"""
        在以下屏幕元素中，找到最匹配"{target}"的元素：
        {screen_state.semantic_description}
        
        返回匹配的元素ID和匹配度。
        """
        result = self.llm.generate(prompt)
        return self.parse_matching_result(result)
```

### 6.2 异常处理策略

```python
class ExceptionHandler:
    def handle(self, exception, context):
        """
        智能异常处理
        LLM自主决策：是记录为bug，还是进行其它操作
        """
        # 1. 捕获屏幕，理解当前状态
        screenshot = self.device_controller.capture_screen()
        screen_state = self.screen_understanding.understand_screen(screenshot)
        
        # 2. LLM分析异常情况
        analysis = self.llm.analyze_exception(
            exception=exception,
            screen_state=screen_state,
            test_case_context=context
        )
        
        # 3. LLM决策处理方式
        if analysis.decision_type == "continue_operation":
            # 情况1：可以继续操作（权限弹窗、业务弹窗等）
            return self.handle_continue_operation(analysis, screen_state)
        elif analysis.decision_type == "go_back":
            # 情况2：需要返回（点击错控件进入错误页面）
            return self.handle_go_back(analysis, screen_state)
        elif analysis.decision_type == "record_bug":
            # 情况3：记录为bug（无法处理或不符合预期）
            return self.handle_record_bug(analysis, screenshot, context)
        else:
            return self.handle_unknown_error(exception, context)
    
    def handle_continue_operation(self, analysis, screen_state):
        """
        处理可以继续操作的异常（权限弹窗、业务弹窗等）
        """
        # LLM已决定处理方式（点击"跳过"、"关闭"、"允许"等）
        target_element = self.element_locator.find_element(
            analysis.target_action, screen_state
        )
        return self.device_controller.execute_action(
            analysis.action_type, target_element
        )
    
    def handle_go_back(self, analysis, screen_state):
        """
        处理需要返回的情况（点击错控件进入错误页面）
        """
        # 1. 执行返回操作
        self.device_controller.press_back()
        
        # 2. 等待页面稳定
        time.sleep(0.5)
        
        # 3. 重新识别屏幕
        new_screen_state = self.screen_understanding.understand_screen(
            self.device_controller.capture_screen()
        )
        
        # 4. 重新定位正确的控件
        return self.retry_find_element(analysis.original_target, new_screen_state)
    
    def handle_record_bug(self, analysis, screenshot, context):
        """
        记录为bug（真正的bug，无法通过操作解决）
        """
        bug_info = {
            "description": analysis.bug_description,
            "screenshot": screenshot,
            "timestamp": datetime.now(),
            "test_case": context.test_case,
            "step": context.current_step,
            "context": analysis.context_info
        }
        self.bug_recorder.record(bug_info)
        
        # 决定是否继续执行或终止测试
        if analysis.should_continue:
            return ContinueExecution()
        else:
            return TerminateTest(bug_info)
```

---

## 七、框架方案选择

### 7.1 为什么选择 APP 形式？

**核心原因：**

1. **系统权限需求**
   - 屏幕捕获需要 `MediaProjection` API，需要应用权限
   - 纯视觉操作，不依赖 `AccessibilityService`
   - 这些系统级权限无法通过其他方式获得

2. **本地 AI 模型部署**
   - AI 模型需要在设备本地运行
   - APP 形式可以完整集成 OCR、UI 检测、LLM 等模型
   - 便于模型管理和更新

3. **用户体验**
   - 提供友好的用户界面（管理用例、查看报告）
   - 可以独立运行，不依赖 PC
   - 支持离线使用

4. **开发便利性**
   - Android SDK 提供完整的开发工具链
   - 调试和测试方便
   - 社区资源丰富

### 7.2 实现方案

**采用：标准 Android APP 形式**

**结构：**
- 标准 Android APP
- 使用 Jetpack Compose UI 框架
- 完整的用户界面

---

## 八、部署架构

### 8.1 完全本地部署方案（工程实践最优解）⭐

**核心原则：所有 AI 模型部署在手机本地，完成完整的用例分析、执行、图像识别和结果输出。**

**工程实践评估**：
- **准确性**：⭐⭐⭐⭐（本地模型，准确率与云端相当）
- **高效性**：⭐⭐⭐⭐（无网络延迟，响应快，预处理后<0.1秒）
- **稳定性**：⭐⭐⭐⭐⭐（不依赖网络，最稳定）
- **隐私安全**：⭐⭐⭐⭐⭐（完全本地，不上传数据）
- **成本**：⭐⭐⭐⭐⭐（一次部署，无API费用）
- **部署复杂度**：⭐⭐⭐（需要本地部署模型，但只需一次）

**架构**：
```
┌─────────────────────────────────────────────────────────┐
│                   测试设备（手机）                         │
│  ┌───────────────────────────────────────────────────┐ │
│  │            TestWings APP                          │ │
│  │  ┌──────────────┐  ┌──────────────┐             │ │
│  │  │  本地AI模型   │  │  设备控制     │             │ │
│  │  │              │  │              │             │ │
│  │  │ - VL模型     │  │ - 屏幕捕获    │             │ │
│  │  │ - LLM        │  │ - 操作执行    │             │ │
│  │  │ - OCR(降级)  │  │ - 结果验证    │             │ │
│  │  └──────────────┘  └──────────────┘             │ │
│  │  ┌──────────────┐  ┌──────────────┐             │ │
│  │  │  测试执行引擎  │  │  数据存储     │             │ │
│  │  │              │  │              │             │ │
│  │  │ - 用例理解    │  │ - SQLite     │             │ │
│  │  │ - 操作规划    │  │ - 结果存储    │             │ │
│  │  │ - 报告生成    │  │ - 截图缓存    │             │ │
│  │  └──────────────┘  └──────────────┘             │ │
│  └───────────────────────────────────────────────────┘ │
│  ┌──────────────┐  ┌──────────────┐                   │
│  │  目标APP     │  │  用户界面    │                   │
│  │  (被测应用)  │  │  (管理界面)  │                   │
│  └──────────────┘  └──────────────┘                   │
└─────────────────────────────────────────────────────────┘

所有功能完全本地执行，无需网络连接
```

**优势**（工程实践角度）：
- ✅ **响应速度快**：无网络延迟，本地推理2-5秒（预处理后<0.1秒）
- ✅ **稳定性最高**：不依赖网络，不受网络波动影响
- ✅ **隐私安全**：完全本地，不上传敏感信息
- ✅ **成本最低**：一次部署，无API费用
- ✅ **离线可用**：不依赖网络，适合内网环境

**结论**：从工程实践角度，完全本地部署是最优解。

### 8.2 企业级混合部署方案（备选方案，不推荐）

**工程实践评估**：
- **准确性**：⭐⭐⭐⭐（云端模型，准确率可能略高，但差异不大）
- **高效性**：⭐⭐（增加网络延迟1-3秒，总响应时间3-8秒，比本地慢）
- **稳定性**：⭐⭐（依赖网络，受网络波动影响，稳定性差）
- **隐私安全**：⭐⭐（需要上传数据到云端，隐私风险高）
- **成本**：⭐⭐（需要API费用或自建服务器，成本高）
- **部署复杂度**：⭐⭐（需要搭建服务器和网络基础设施，复杂度高）

**适用场景**：仅在以下极端情况下考虑
- 需要最新模型能力（本地模型无法满足）
- 可以接受网络延迟和隐私风险
- 有充足的预算和基础设施

**工程实践结论**：
- ❌ **不推荐**：从工程实践角度，混合部署的劣势远大于优势
- ✅ **推荐**：完全本地部署是最优解，除非有特殊需求

```
┌─────────────────────────────────────────────────────────┐
│                     企业级 LLM 服务器                     │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  用例理解    │  │  屏幕理解    │  │  操作规划    │  │
│  │  LLM        │  │  Vision-LM   │  │  LLM        │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
│  ┌──────────────┐  ┌──────────────┐                   │
│  │  Web服务     │  │  数据存储    │                   │
│  │  (FastAPI)   │  │  (PostgreSQL)│                   │
│  └──────────────┘  └──────────────┘                   │
└────────────────────┬────────────────────────────────────┘
                     │
                     │ HTTP/WebSocket
                     │
┌────────────────────▼────────────────────────────────────┐
│           多个 TestWings APP (手机端)                     │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  本地模型    │  │  设备控制     │                   │
│  │              │  │              │                   │
│  │ - VL模型     │  │ - 屏幕捕获    │                   │
│  │ - LLM        │  │ - 操作执行    │                   │
│  │ - OCR(可选)  │  │              │                   │
│  └──────────────┘  └──────────────┘                   │
└─────────────────────────────────────────────────────────┘
```

**优势：**
- ✅ 集中管理，统一更新模型
- ✅ 支持大规模并发
- ✅ 降低单个设备内存压力

---

## 九、性能优化策略

### 9.1 识别性能优化

1. **增量识别**：只识别变化区域
2. **缓存机制**：缓存UI元素识别结果
3. **模型量化**：使用量化模型减少计算量
4. **并行处理**：屏幕识别和操作执行并行

### 9.2 网络优化

1. **本地模型优先**：优先使用本地模型，减少网络请求
2. **批量请求**：合并多个AI请求
3. **结果缓存**：缓存AI分析结果
4. **压缩传输**：压缩屏幕截图和传输数据

### 9.3 执行效率优化

1. **智能等待**：根据UI状态智能等待，而非固定延时
2. **操作合并**：合并连续操作，减少屏幕刷新
3. **失败快速恢复**：快速识别失败并重试

---

## 十、安全与隐私

### 10.1 数据安全

1. **加密传输**：所有数据传输使用HTTPS/WSS
2. **数据加密存储**：敏感数据加密存储
3. **访问控制**：基于角色的访问控制（RBAC）

### 10.2 隐私保护

1. **本地处理优先**：优先使用本地模型，减少数据上传
2. **数据脱敏**：上传前对敏感信息脱敏
3. **用户控制**：用户可控制数据上传范围
4. **数据保留策略**：自动清理过期数据

---

## 十一、扩展性设计

### 11.1 插件系统

支持插件扩展功能：
- **自定义识别器**：支持自定义UI元素识别器
- **自定义操作**：支持自定义操作类型
- **自定义验证器**：支持自定义验证逻辑

### 11.2 多平台支持

- **Android**：完整支持
- **iOS**：基础支持（受系统限制）
- **Web**：通过浏览器自动化扩展支持

### 11.3 多语言支持

- **测试用例**：支持多语言测试用例
- **UI识别**：支持多语言UI识别
- **报告生成**：支持多语言报告

---

## 十二、开发路线图

### Phase 1: 基础能力（当前）✅
- ✅ 屏幕捕获
- ✅ OCR识别（降级方案）
- ✅ 基础操作执行（纯坐标操作）

### Phase 2: Vision-Language集成 ⏳
- ⏳ 集成Qwen-VL模型（核心能力，必需）
- ⏳ 实现屏幕内容理解
- ⏳ 实现元素识别和定位（VL综合定位）
- ⏳ 实现预处理机制（必需的性能优化，提升执行效率10-50倍）

### Phase 3: LLM策略规划 ⏳
- ⏳ 集成Qwen-7B模型
- ⏳ 实现用例理解
- ⏳ 实现策略规划
- ⏳ 实现自主决策

### Phase 4: 完整流程 ⏳
- ⏳ 支持自然语言用例
- ⏳ 实现完整测试流程
- ⏳ 实现意外情况处理

---

## 十三、总结

### 13.1 核心价值

1. **纯视觉操作**：完全基于屏幕截图，不依赖系统服务
2. **AI自主决策**：大模型参与每一步决策，像人一样思考
3. **自然语言用例**：支持自然语言描述测试用例
4. **智能容错**：能够处理意外情况，智能决策

### 13.2 核心特点

**方案特点**：Vision-Language + LLM + AI自主决策 + 纯视觉操作

---

**详细技术实现**：参见 `技术实现方案.md`
**模型部署方案**：参见 `模型部署方案.md`
**预处理机制**：参见 `预处理机制.md`
- [x] 屏幕捕获功能
- [x] 基础操作执行（点击、滑动、输入）
- [ ] Vision-Language模型集成（屏幕理解）
- [ ] 简单用例执行
- [ ] 基础报告生成

### Phase 2: AI增强
- [ ] LLM集成（用例理解、操作规划）
- [ ] AI自主决策能力
- [ ] 预处理机制实现
- [ ] 异常处理优化

### Phase 3: 完善功能
- [ ] 复杂操作支持（滑动、手势）
- [ ] 结果验证增强
- [ ] 报告完善

### Phase 4: 优化与扩展
- [ ] 性能优化
- [ ] 插件系统
- [ ] 多平台支持
- [ ] 企业级功能

