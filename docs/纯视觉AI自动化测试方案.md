# 纯视觉AI自动化测试方案

## 一、核心设计理念

### 1.1 设计目标

**TestWings 是一个完全基于视觉AI的自动化测试框架**，核心特点：

1. **纯视觉操作**：完全依赖屏幕截图和AI视觉理解，不依赖Accessibility Service
2. **AI自主决策**：大模型根据用例和当前屏幕状态，自主制定操作策略
3. **类人操作**：像人一样观察屏幕、理解内容、做出决策、执行操作
4. **实时响应**：实时获取屏幕信息，实时决策，实时执行

### 1.2 核心能力

- **视觉理解**：理解屏幕上的所有元素（文字、按钮、图标、布局等）
- **语义理解**：理解用例意图和屏幕内容的语义
- **策略规划**：根据用例和屏幕状态，制定操作步骤
- **自主决策**：遇到意外情况（弹窗、广告等），智能决策处理方式
- **操作执行**：将决策转化为具体的操作（点击、滑动、输入等）
- **预处理机制**：预先处理页面截图，提升执行速度和准确率

---

## 二、当前方案分析

### 2.1 当前架构特点

**当前方案**：
- 基于OCR + Accessibility Service
- 测试用例是结构化的JSON/YAML
- 元素定位依赖OCR文本匹配或Resource ID
- 操作执行依赖Accessibility Service

**与目标方案的差异**：
- ❌ 依赖Accessibility Service（非纯视觉）
- ❌ 用例是结构化的，不是自然语言
- ❌ 决策逻辑是硬编码的，不是AI自主决策
- ✅ 已有OCR识别能力
- ✅ 已有屏幕捕获能力
- ✅ 已有操作执行能力

### 2.2 需要改进的方向

1. **移除Accessibility Service依赖**：完全基于视觉定位
2. **支持自然语言用例**：AI理解自然语言用例
3. **AI自主决策**：大模型参与每一步决策
4. **视觉元素识别**：不仅识别文字，还要识别按钮、图标、布局等

---

## 三、目标架构设计

### 3.1 整体架构

```
┌─────────────────────────────────────────┐
│        用户输入层                         │
│  - 自然语言用例                           │
│  - JSON/YAML用例（可选）                  │
└─────────────────┬───────────────────────┘
                  │
┌─────────────────▼───────────────────────┐
│        预处理层（可选，提升性能）          │
│  - 页面截图预处理                         │
│  - VL模型识别所有元素                     │
│  - 预处理结果缓存                         │
└─────────────────┬───────────────────────┘
                  │
┌─────────────────▼───────────────────────┐
│        AI核心引擎层（大模型）             │
│  ┌───────────────────────────────────┐ │
│  │  用例理解模块                       │ │
│  │  - 理解用例意图                     │ │
│  │  - 生成测试计划                     │ │
│  └───────────────────────────────────┘ │
│  ┌───────────────────────────────────┐ │
│  │  屏幕理解模块（Vision-Language）    │ │
│  │  - 理解屏幕内容                     │ │
│  │  - 识别所有元素                     │ │
│  │  - 理解布局和语义                   │ │
│  └───────────────────────────────────┘ │
│  ┌───────────────────────────────────┐ │
│  │  策略规划模块（LLM）                │ │
│  │  - 根据用例和屏幕状态制定策略        │ │
│  │  - 决定下一步操作                   │ │
│  │  - 处理意外情况                     │ │
│  └───────────────────────────────────┘ │
└─────────────────┬───────────────────────┘
                  │
┌─────────────────▼───────────────────────┐
│        视觉识别层（VL为主）                │
│  - Vision-Language模型（主要）            │
│  - OCR文字识别（降级方案，可选）          │
│  - 图像预处理                             │
└─────────────────┬───────────────────────┘
                  │
┌─────────────────▼───────────────────────┐
│        操作执行层                         │
│  - 屏幕坐标点击                           │
│  - 滑动操作                               │
│  - 文本输入                               │
└─────────────────────────────────────────┘
```

### 3.2 核心流程

```
1. 预处理阶段（可选，提升性能）
   a. 准备页面截图（静态页面或滚动截图）
   b. VL模型预处理，识别所有元素
   c. 存储预处理结果
   ↓
2. 用户输入用例（自然语言或结构化）
   ↓
3. AI理解用例，生成测试计划
   ↓
4. 循环执行测试步骤：
   a. 捕获当前屏幕
   b. 匹配预处理结果
      ├─ 匹配成功：使用预处理结果
      └─ 未匹配：VL实时识别（OCR作为降级）
   c. AI根据用例和屏幕状态制定策略（LLM）
   d. AI决定下一步操作（点击哪里、输入什么等）
   e. 执行操作
   f. 验证结果
   g. 处理意外情况（弹窗、广告等）
   ↓
5. 生成测试报告
```

---

## 四、关键技术模块

### 4.1 预处理模块（可选，提升性能）

**功能**：
- 预处理页面截图，识别所有元素
- 支持静态页面和滚动截图
- 生成结构化预处理结果

**技术方案**：
- 使用Vision-Language模型预处理
- 存储预处理结果（本地或云端）

**优势**：
- 提升执行速度（直接使用预处理结果）
- 提高准确率（完整页面截图）
- 减少实时计算开销

**详细设计**：参见 `docs/预处理机制设计.md`

### 4.2 视觉理解模块（Vision-Language Model）

**功能**：
- 理解屏幕上的所有内容
- 识别文字、按钮、图标、布局
- 理解元素之间的关系和语义

**技术方案**：
- **主要方案**：本地部署 Qwen-VL（INT8量化）
- **备选方案**：云端API（GPT-4V、Claude Vision）
- **降级方案**：OCR（仅在VL不可用时使用）

**策略**：
- **VL为主**：主要使用VL模型进行屏幕理解
- **OCR为辅**：仅作为降级方案（可选）

**输入**：
- 屏幕截图
- 用例描述（当前步骤）

**输出**：
- 屏幕状态描述（结构化）
- 所有可操作元素列表（位置、类型、内容）

**详细分析**：参见 `docs/OCR vs VL选择分析.md`

### 4.2 策略规划模块（LLM）

**功能**：
- 根据用例和屏幕状态，制定操作策略
- 决定下一步操作
- 处理意外情况

**技术方案**：
- 本地部署：Qwen-7B（INT8量化）
- 或云端API：GPT-4、Claude

**输入**：
- 用例描述
- 当前屏幕状态
- 历史操作记录

**输出**：
- 下一步操作指令（操作类型、目标元素、参数）

### 4.3 用例理解模块（LLM）

**功能**：
- 理解自然语言用例
- 生成结构化的测试计划

**技术方案**：
- 与策略规划模块共享同一个LLM模型

---

## 五、典型场景处理

### 5.1 场景示例（记录，不细节化）

1. **打开指定APP**
   - AI识别屏幕上的所有APP图标
   - 根据用例要求找到目标APP
   - 如果不在当前屏幕，滑动查找
   - 找到后点击打开

2. **处理广告页面**
   - AI识别广告特征
   - 查找"跳过"、"X"等关闭按钮
   - 自动点击关闭

3. **处理更新弹窗**
   - AI识别更新弹窗
   - 根据用例要求决定点击"更新"或"忽略"

4. **处理权限弹窗**
   - AI识别权限请求
   - 根据测试场景智能决策是否允许

5. **处理意外弹窗**
   - AI识别意外弹窗
   - 根据测试场景和弹窗内容智能决策处理方式

**注意**：这些场景只是示例，实际AI会根据具体情况自主决策，不需要硬编码处理逻辑。

---

## 六、技术选型

### 6.1 模型选择

**视觉理解**：
- 优先：Qwen-VL（本地部署）
- 备选：GPT-4V、Claude Vision（云端API）

**策略规划**：
- 优先：Qwen-7B（本地部署）
- 备选：GPT-4、Claude（云端API）

### 6.2 部署方案

**完全本地部署（推荐）**：
- 所有模型部署在手机本地
- 隐私安全，无需网络
- 响应速度快

**混合部署（可选）**：
- 轻量级模型本地部署
- 大模型云端部署

---

## 七、实施路线图

### Phase 1: 基础能力（当前）
- ✅ 屏幕捕获
- ✅ OCR识别
- ✅ 基础操作执行
- ⏳ OCR准确率优化

### Phase 2: 视觉理解能力
- ⏳ 集成Vision-Language模型
- ⏳ 实现屏幕内容理解
- ⏳ 实现元素识别和定位

### Phase 3: AI决策能力
- ⏳ 集成LLM模型
- ⏳ 实现用例理解
- ⏳ 实现策略规划
- ⏳ 实现自主决策

### Phase 4: 完整流程
- ⏳ 支持自然语言用例
- ⏳ 实现完整测试流程
- ⏳ 实现意外情况处理
- ⏳ 优化性能和准确率

---

## 八、与当前方案的对比

| 维度 | 当前方案 | 目标方案 |
|------|---------|---------|
| 用例格式 | JSON/YAML | 自然语言 + JSON/YAML |
| 元素定位 | OCR + Resource ID | 纯视觉定位 |
| 决策方式 | 硬编码逻辑 | AI自主决策 |
| 依赖服务 | Accessibility Service | 无依赖（纯视觉） |
| 意外处理 | 无 | AI智能处理 |
| 核心能力 | OCR + 操作执行 | Vision-Language + LLM |

---

## 九、总结

### 9.1 核心价值

1. **纯视觉操作**：完全基于屏幕截图，不依赖系统服务
2. **AI自主决策**：大模型参与每一步决策，像人一样思考
3. **自然语言用例**：支持自然语言描述测试用例
4. **智能容错**：能够处理意外情况，智能决策

### 9.2 关键差异

**当前方案**：OCR + Accessibility Service + 硬编码逻辑
**目标方案**：Vision-Language + LLM + AI自主决策

**核心转变**：从"工具辅助"到"AI主导"

---

**下一步**：详细设计各个模块的实现方案和技术细节。

