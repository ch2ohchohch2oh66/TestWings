# TestWings 模型部署方案（完全本地部署优先）

> **文档定位**：本文档是**模型选型和部署的唯一权威文档**，包含所有关于VL/LLM模型选型、为什么需要分别部署、部署方案、工程实践评估、API服务选型等内容。其他文档中的模型选型相关内容应引用本文档，避免重复。

> **相关文档**：
> - `01-01-架构设计.md` - 系统整体架构（包含模型选型的简要说明）
> - `01-02-纯视觉AI技术实现方案.md` - 纯视觉AI技术的详细实现（如何使用VL/LLM）

## 一、部署策略

### 1.1 核心原则

**优先完全本地部署：**
- 所有 AI 模型部署在手机本地
- 完成完整的用例分析、执行、图像识别和结果输出
- 降低测试环境复杂度，无需搭建服务器
- 保护数据隐私，不上传敏感信息

**企业级场景可选混合部署：**
- 针对企业级大规模使用场景
- 可单独搭建 LLM 模型服务器
- 支持云端和本地混合部署模式

### 1.2 整体架构

#### 完全本地部署架构（优先方案）

```
┌─────────────────────────────────────────────────────────┐
│               TestWings APP (手机端)                      │
│  ┌───────────────────────────────────────────────────┐ │
│  │           完全本地 AI 模型层                       │ │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐        │ │
│  │  │ 屏幕理解 │  │ 用例理解 │  │ 操作规划 │        │ │
│  │  │ Vision-  │  │ 本地LLM  │  │ 本地LLM  │        │ │
│  │  │ Language │  │ (量化)   │  │ (量化)   │        │ │
│  │  │ (主要)⭐ │  │          │  │          │        │ │
│  │  └──────────┘  └──────────┘  └──────────┘        │ │
│  │  ┌──────────┐                                      │ │
│  │  │  OCR     │  (降级方案，可选)                    │ │
│  │  │ ML Kit   │                                      │ │
│  │  └──────────┘                                      │ │
│  └───────────────────────────────────────────────────┘ │
│                                                        │
│  所有功能完全本地执行，无需网络连接                      │
└─────────────────────────────────────────────────────────┘
```

#### 混合部署架构（企业级可选方案）

```
┌─────────────────────────────────────────────────────────┐
│               TestWings APP (手机端)                      │
│  ┌───────────────────────────────────────────────────┐ │
│  │           本地轻量级模型层                          │ │
│  │  ┌──────────┐                                      │ │
│  │  │  OCR     │  (降级方案，可选)                    │ │
│  │  │ ML Kit   │                                      │ │
│  │  └──────────┘                                      │ │
│  └───────────────────────────────────────────────────┘ │
│                        ↕ HTTP/WebSocket                  │
│  ┌───────────────────────────────────────────────────┐ │
│  │           企业级 LLM 服务器                        │ │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐        │ │
│  │  │ 用例理解 │  │ 屏幕理解 │  │ 操作规划 │        │ │
│  │  │ LLM      │  │ Vision-  │  │ LLM      │        │ │
│  │  │          │  │ Language │  │          │        │ │
│  │  └──────────┘  └──────────┘  └──────────┘        │ │
│  └───────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────┘
```

### 1.3 模型分工（完全本地部署）

| 任务 | 模型类型 | 部署位置 | 内存占用 | 推理时间 | 模型大小 | 优先级 |
|------|---------|---------|---------|---------|---------|--------|
| 屏幕理解 | Vision-Language（主要）⭐ | 本地 | ~4-6GB | 3-8s | ~4-7GB | **高** |
| 用例理解 | 量化LLM | 本地 | ~4-6GB | 2-5s | ~4-7GB | **高** |
| 操作规划 | 量化LLM | 本地 | ~4-6GB | 1-3s | ~4-7GB | **高** |
| 文字识别 | OCR（降级） | 本地 | ~100MB | < 1s | ~20MB | **低** |

**关键说明**：

**1. 为什么需要分别部署VL和LLM？**

**多模态AI应用的架构（如Cursor、豆包）**：
- 这些应用通常使用**单一的多模态大模型**（如GPT-4V、Claude Vision、Qwen-VL）
- 这些模型可以同时处理图像和文本，输出自然语言描述
- **适用场景**：图像理解、对话、问答等，输出是自然语言文本

**自动化测试的特殊需求**：
- **结构化输出**：需要精确的元素位置坐标（x, y, width, height），用于点击操作
- **高精度要求**：需要识别每个UI元素的边界框和中心坐标，精度要求高
- **职责分离**：
  - **VL模型**：专门负责"看"（视觉理解），输出结构化的元素列表和坐标
  - **LLM模型**：专门负责"想"（语义理解和决策），基于VL的结构化数据进行用例理解、操作规划

**2. VL和LLM的职责分工**：
- **VL模型**：专门负责屏幕理解，识别所有UI元素（按钮、输入框等）及其精确位置坐标，输出结构化数据
- **LLM模型**：负责用例理解、操作规划、自主决策，基于VL识别的结构化数据进行语义理解和策略规划
- **两者需要同时部署**：职责不同，不能相互替代

**3. 技术实现差异**：
- **多模态LLM（如Qwen-VL）**：虽然可以处理图像，但通常输出自然语言描述，难以保证结构化输出的精度
- **专门的VL模型**：专门为视觉理解设计，可以输出结构化的JSON数据（元素类型、位置、边界框等）
- **纯文本LLM（如Qwen-7B）**：无法处理图像输入，只能处理文本

**结论**：
- **一般AI应用**：可以使用单一的多模态模型（如GPT-4V、Qwen-VL），输出自然语言
- **自动化测试**：需要分别部署VL和LLM，VL负责结构化输出，LLM负责语义理解和决策
- **实际上，Qwen-VL本身就是多模态模型**，但我们需要它专门负责视觉理解的结构化输出，而LLM负责基于这些结构化数据进行决策
- **VL为主**：Vision-Language模型是屏幕理解的主要能力，必需部署
- **OCR为辅**：OCR仅作为VL模型不可用时的降级方案
- **LLM共享**：用例理解、操作规划可以共享同一个LLM模型

---

## 二、本地模型部署方案

### 2.1 Vision-Language模型：Qwen-VL-Chat（主要能力）⭐

#### 模型信息
- **模型名称**：Qwen2-VL-2B-Instruct（Q4F16量化）⭐
- **模型大小**：约 2.1GB（解码器829MB + 视觉编码器1.27GB）
- **内存占用**：约 3-4GB（运行时）
- **注意**：INT8量化版本使用ConvInteger操作符，ONNX Runtime Android不支持，因此使用Q4F16版本
- **推理速度**：
  - CPU：3-8秒（单张图片）
  - NPU：1-3秒（如果支持）

#### 核心能力
- ✅ 同时理解图像和文本
- ✅ 识别所有UI元素（文字、按钮、输入框等）
- ✅ 提取元素坐标和语义描述
- ✅ 理解屏幕布局和上下文

#### 集成方式

```kotlin
// 1. 添加依赖（build.gradle）
dependencies {
    // ONNX Runtime（用于模型推理）
    implementation 'com.microsoft.onnxruntime:onnxruntime-android:1.20.0'
}

// 2. 初始化VL模型
class VisionLanguageManager {
    private var vlModel: QwenVLModel? = null
    
    suspend fun init(context: Context) {
        vlModel = QwenVLModel(context)
        vlModel?.loadModel("qwen_vl_chat_int8.onnx")
    }
    
    suspend fun understand(screenshot: Bitmap): ScreenState {
        val result = vlModel?.understand(screenshot)
        return ScreenState(
            elements = result?.elements ?: emptyList(),
            semanticDescription = result?.description ?: ""
        )
    }
}
```

#### 模型文件结构

**注意**：模型文件现在放在外部存储，不在assets中（避免APK过大）。

```
/sdcard/Android/data/com.testwings/files/models/vl/
├── decoder_model_merged_q4f16.onnx  # Qwen2-VL-2B解码器模型（Q4F16量化，约829MB，推荐）⭐
│   # 或 decoder_model_merged.onnx  # 未量化版本（不推荐，文件更大）
│   # 注意：decoder_model_merged_int8.onnx 不支持（ONNX Runtime Android不支持ConvInteger操作符）
├── vision_encoder_q4f16.onnx  # Qwen2-VL-2B视觉编码器模型（Q4F16量化，约1.27GB，推荐）⭐
│   # 或 vision_encoder.onnx  # 未量化版本（不推荐，文件更大）
│   # 注意：vision_encoder_int8.onnx 不支持（ONNX Runtime Android不支持ConvInteger操作符）
├── config.json                      # 模型配置
├── preprocessor_config.json         # 预处理器配置
└── tokenizer.json                   # 分词器
```

详细部署步骤请参考：[Qwen2-VL模型部署操作手册](../docs/01-05-Qwen2-VL模型部署操作手册.md)

---

### 2.2 LLM模型：Qwen-7B-Chat（用例理解和操作规划）

#### 模型选择

**推荐模型（按优先级）：**

1. **Qwen-7B-Chat（Q4F16 量化）** ⭐ 推荐
   - **模型大小**：约 4-5GB（Q4F16 量化）
   - **内存占用**：约 4-6GB（运行时）
   - **注意**：INT8量化版本可能使用ConvInteger操作符，ONNX Runtime Android可能不支持，建议使用Q4F16版本
   - **推理速度**：
     - CPU：5-10秒
     - NPU：2-5秒（如果支持）
   - **用途**：用例理解、操作规划（屏幕理解由VL模型负责）

2. **Qwen-1.8B-Chat（Q4F16 量化）** - 轻量级选择
   - **模型大小**：约 1-2GB（Q4F16 量化）
   - **内存占用**：约 1.5-2.5GB（运行时）
   - **注意**：INT8量化版本可能使用ConvInteger操作符，ONNX Runtime Android可能不支持，建议使用Q4F16版本
   - **推理速度**：
     - CPU：2-4秒
     - NPU：< 2秒（如果支持）
   - **用途**：简单用例理解、操作规划

3. **Llama-2-7B-Chat（Q4F16 量化）**
   - **模型大小**：约 4-5GB（Q4F16 量化）
   - **内存占用**：约 4-6GB（运行时）
   - **注意**：INT8量化版本可能使用ConvInteger操作符，ONNX Runtime Android可能不支持，建议使用Q4F16版本
   - **推理速度**：类似 Qwen-7B

#### 模型量化

**Q4F16 量化优势：**
- 模型大小减少约 50-60%（相比未量化版本）
- 内存占用减少约 50-60%
- **兼容性最好**：使用标准浮点操作符，ONNX Runtime Android完全支持
- **注意**：INT8量化版本使用ConvInteger操作符，ONNX Runtime Android不支持，因此推荐使用Q4F16版本
- 推理速度提升 2-4 倍
- 准确率损失 < 5%

**量化工具：**
- ONNX Quantization
- TensorFlow Lite Quantization
- PaddlePaddle Quantization

#### 集成方式

**使用 ONNX Runtime：**

```kotlin
dependencies {
    implementation 'com.microsoft.onnxruntime:onnxruntime-android:1.16.0'
}

class LLMManager {
    private var ortEnv: OrtEnvironment? = null
    private var ortSession: OrtSession? = null
    
    fun init(context: Context) {
        ortEnv = OrtEnvironment.getEnvironment()
        val modelBytes = loadModel(context, "qwen_7b_int8.onnx")
        val sessionOptions = OrtSession.SessionOptions()
        
        // 配置优化选项
        sessionOptions.setIntraOpNumThreads(4) // 使用 4 个线程
        // sessionOptions.addExecutionProvider("NNAPI") // 使用 NPU（如果支持）
        
        ortSession = ortEnv?.createSession(modelBytes, sessionOptions)
    }
    
    suspend fun generate(
        prompt: String,
        maxTokens: Int = 512,
        temperature: Float = 0.7f
    ): String {
        // Tokenize
        val tokens = tokenize(prompt)
        
        // 推理
        val inputs = mapOf("input_ids" to tokens)
        val outputs = ortSession?.run(inputs)
        
        // Decode
        return decode(outputs)
    }
}
```

#### 模型文件结构
```
app/src/main/assets/models/llm/
├── qwen_7b_int8.onnx          # 量化后的 LLM 模型（约 4-5GB）
├── tokenizer.json             # Tokenizer 配置
└── config.json                # 模型配置
```

### 2.3 OCR模型：ML Kit Text Recognition（降级方案，可选）

#### 模型信息
- **模型名称**：ML Kit Text Recognition
- **模型大小**：约 20MB（内置在ML Kit中）
- **内存占用**：约 100MB（运行时）
- **推理速度**：
  - CPU：< 1秒（单张图片）
  - NPU：< 0.3秒（如果支持）

#### 核心能力
- ✅ 识别屏幕上的文字内容
- ✅ 提取文字位置坐标
- ✅ 支持多语言识别

#### 集成方式

```kotlin
// 1. 添加依赖（build.gradle）
dependencies {
    implementation 'com.google.mlkit:text-recognition:16.0.0'
}

// 2. OCR管理器
class OCRManager {
    private val textRecognizer = TextRecognition.getClient()
    
    suspend fun recognize(screenshot: Bitmap): List<TextElement> {
        val image = InputImage.fromBitmap(screenshot, 0)
        val result = textRecognizer.process(image).await()
        
        return result.textBlocks.map { block ->
            TextElement(
                text = block.text,
                bbox = block.boundingBox,
                confidence = 0.9f // ML Kit不提供置信度，使用默认值
            )
        }
    }
}
```

---

## 三、模型优化策略

### 3.1 NPU 加速（华为设备）

**使用 HiAI 框架或 NNAPI：**

```kotlin
class NPUAccelerator {
    fun optimizeForNPU(modelPath: String): String {
        // 使用 HiAI 转换模型
        val converter = HiAIModelConverter()
        val optimizedModel = converter.convert(
            modelPath = modelPath,
            targetDevice = "NPU",
            quantization = "INT8"
        )
        return optimizedModel
    }
}

// 在 ONNX Runtime 中使用 NNAPI
val sessionOptions = OrtSession.SessionOptions()
sessionOptions.addExecutionProvider("NNAPI") // 启用 NPU 加速
```

**NPU 加速效果：**
- 推理速度提升 5-10 倍
- 降低 CPU 负载和功耗
- 支持 INT8 量化模型

### 3.2 模型缓存与预加载

```kotlin
class ModelManager {
    private val modelCache = mutableMapOf<String, Any>()
    
    fun preloadModels(context: Context) {
        // 在后台线程预加载模型
        GlobalScope.launch(Dispatchers.IO) {
            // 预加载 OCR 模型（轻量级，优先加载）
            loadOCRModel(context)
            
            // 预加载 UI 检测模型
            loadUIDetectorModel(context)
            
            // 延迟加载 LLM（较大，按需加载）
            // loadLLMModel(context)
        }
    }
    
    fun loadLLMModelOnDemand(context: Context) {
        if (!modelCache.containsKey("llm")) {
            GlobalScope.launch(Dispatchers.IO) {
                loadLLMModel(context)
            }
        }
    }
    
    private fun hasEnoughMemory(): Boolean {
        val runtime = Runtime.getRuntime()
        val freeMemory = runtime.freeMemory()
        val totalMemory = runtime.totalMemory()
        val availableMemory = runtime.maxMemory() - (totalMemory - freeMemory)
        
        // 需要至少 6GB 可用内存才加载 LLM
        return availableMemory > 6 * 1024 * 1024 * 1024
    }
}
```

### 3.3 模型共享优化

**用例理解、操作规划、屏幕理解可以共享同一个 LLM：**

```kotlin
class SharedLLMManager {
    private var llmModel: OrtSession? = null
    
    fun init(context: Context) {
        // 只加载一个 LLM 模型
        llmModel = loadLLMModel(context, "qwen_7b_int8.onnx")
    }
    
    suspend fun understandTestCase(testCase: String): TestPlan {
        val prompt = buildTestCasePrompt(testCase)
        return generate(prompt, taskType = "understand")
    }
    
    suspend fun planOperation(step: TestStep, screen: ScreenState): OperationPlan {
        val prompt = buildOperationPrompt(step, screen)
        return generate(prompt, taskType = "plan")
    }
    
    suspend fun understandScreen(screenshot: Bitmap): ScreenState {
        val prompt = buildScreenPrompt(screenshot)
        return generate(prompt, taskType = "understand_screen")
    }
    
    private suspend fun generate(prompt: String, taskType: String): Any {
        // 使用同一个模型，通过不同的 prompt 实现不同功能
        val tokens = tokenize(prompt)
        val inputs = mapOf("input_ids" to tokens)
        val outputs = llmModel?.run(inputs)
        return parseOutput(outputs, taskType)
    }
}
```

---

## 四、硬件配置要求

### 4.1 完全本地部署配置要求

#### 最低配置（8GB 内存）
- **内存**：8GB RAM
- **存储**：64GB+（用于存储模型文件）
- **处理器**：支持 NPU 推荐

**可部署模型：**
- OCR 模型：✅
- UI 检测模型：✅
- Qwen-1.8B（INT8）：✅（约 1.5-2.5GB 内存）
- Qwen-7B（INT8）：⚠️（可能内存不足，需要优化）

**总内存占用估算：**
- 系统：~2-3GB
- OCR：~100MB
- UI 检测：~200MB
- LLM（1.8B）：~1.5-2.5GB
- APP 运行：~500MB
- **总计**：约 4.5-6.5GB（接近极限）

#### 推荐配置（12GB+ 内存）
- **内存**：12GB+ RAM（推荐 16GB）
- **存储**：128GB+（用于存储模型文件）
- **处理器**：支持 NPU（如麒麟 990/9000、骁龙 8 Gen 2+）

**可部署模型：**
- OCR 模型：✅
- UI 检测模型：✅
- Qwen-7B（INT8）：✅（约 4-6GB 内存）
- Qwen-VL（INT8）：✅（约 4-6GB 内存，与 LLM 共享）

**总内存占用估算：**
- 系统：~2-3GB
- OCR：~100MB
- UI 检测：~200MB
- LLM（7B）：~4-6GB
- APP 运行：~500MB
- **总计**：约 7-10GB（12GB 内存充足）

### 4.2 模型选择建议

**8GB 内存设备：**
- 使用 Qwen-1.8B（INT8）进行用例理解和操作规划
- 屏幕理解可以结合 OCR + UI 检测 + 规则引擎，不必须 Vision-Language 模型

**12GB+ 内存设备：**
- 使用 Qwen-7B（INT8）进行所有 LLM 任务
- 可选部署 Qwen-VL 进行复杂屏幕理解

---

## 五、API云端部署方案（DEMO验证和快速开发）⭐

> **说明**：由于本地部署VL模型遇到ONNX Runtime限制问题，采用API云端部署方案可以立即开始DEMO验证，不阻塞项目进度。

### 5.1 适用场景

**适用场景**：
- DEMO验证和快速开发
- 本地部署遇到技术限制（如ONNX Runtime问题）
- 需要快速验证LLM+VL可行性
- 成本敏感场景（使用性价比高的API服务）

### 5.2 选型原则

1. **网络访问**：墙内可直接访问，无需VPN
2. **模型能力**：支持中文，理解能力强，适合测试用例理解和操作规划
3. **性价比**：价格合理，适合DEMO验证和后续扩展
4. **API稳定性**：服务稳定，响应速度快
5. **VL模型支持**：优先选择同时提供LLM和VL模型的厂商，降低集成复杂度

### 5.3 候选服务对比

#### 5.3.1 阿里云通义千问 ⭐ **主方案（推荐）**

**LLM服务**：
- **模型**：Qwen-Max（旗舰）、Qwen-Plus、Qwen-Turbo
- **价格**（2024年）：
  - Qwen-Max：输入 0.04元/千Tokens
  - Qwen-Plus：输入 0.012元/千Tokens（推荐，平衡性能和价格）
  - Qwen-Turbo：输入 0.002元/千Tokens（快速验证）
- **特点**：
  - ✅ 墙内直接访问，网络稳定
  - ✅ 中文能力强，支持长文本
  - ✅ 兼容OpenAI接口规范，易于集成
  - ✅ 提供免费额度（新用户）

**VL模型服务**：
- **模型**：Qwen-VL、Qwen-VL-Max
- **特点**：
  - ✅ 支持图文识别、描述、问答
  - ✅ 支持视觉定位和图像中文字理解
  - ✅ 兼容OpenAI接口规范
  - ✅ 与LLM服务同一平台，集成简单

**API文档**：
- 百炼平台：https://help.aliyun.com/zh/model-studio/
- 兼容OpenAI：https://help.aliyun.com/zh/model-studio/qwen-vl-compatible-with-openai

**推荐指数**：⭐⭐⭐⭐⭐

---

#### 5.3.2 DeepSeek ⭐ **备选方案（性价比首选）**

**LLM服务**：
- **模型**：DeepSeek-V2、DeepSeek-Chat
- **价格**（2024年）：
  - DeepSeek-V2：输入 0.001元/千Tokens，输出 0.002元/千Tokens
  - **价格极低，对标GPT-4 Turbo但价格为其1%**
- **特点**：
  - ✅ 墙内直接访问
  - ✅ 性价比极高
  - ✅ 中文能力强
  - ✅ 提供免费额度

**VL模型服务**：
- **模型**：DeepSeek-VL（开源，1.3B和7B版本）
- **特点**：
  - ✅ 开源模型，可本地部署
  - ⚠️ API服务可能不完善，需要确认

**API文档**：
- DeepSeek API：https://platform.deepseek.com/

**推荐指数**：⭐⭐⭐⭐（LLM性价比极高，VL需确认）

---

#### 5.3.3 其他候选服务

**百度文心一言**：
- LLM：ERNIE-Bot（0.012元/千Tokens）、ERNIE-Bot-turbo（0.008元/千Tokens）
- VL：ERNIE-Bot 5.0（原生全模态架构）
- 推荐指数：⭐⭐⭐⭐

**智谱GLM**：
- LLM：GLM-3 Turbo（0.001元/千Tokens，入门级）
- VL：支持不明确
- 推荐指数：⭐⭐⭐

---

### 5.4 推荐方案

#### 方案一：通义千问（主方案）⭐

**配置**：
- **LLM**：Qwen-Plus（平衡性能和价格）或 Qwen-Turbo（快速验证）
- **VL**：Qwen-VL-Max（高精度）或 Qwen-VL（性价比）

**优势**：
- ✅ LLM和VL模型在同一平台，集成简单
- ✅ 兼容OpenAI接口，代码迁移成本低
- ✅ 网络稳定，墙内直接访问
- ✅ 提供免费额度，适合DEMO验证
- ✅ 中文能力强，适合测试用例理解

**适用场景**：
- DEMO验证
- 生产环境
- 需要稳定服务

---

#### 方案二：DeepSeek + 通义千问VL（备选方案）⭐

**配置**：
- **LLM**：DeepSeek-V2（用例理解、操作规划，成本低）
- **VL**：通义千问VL（屏幕理解，能力稳定）

**优势**：
- ✅ 成本最低（LLM用DeepSeek，VL用通义千问）
- ✅ 能力互补（DeepSeek LLM能力强，通义千问VL稳定）
- ✅ 降低单点依赖风险

**适用场景**：
- 成本敏感场景
- DEMO验证
- 需要大量调用

---

### 5.5 实施建议

**第一步：选择主方案（推荐：通义千问）**

**理由**：
1. LLM和VL在同一平台，降低集成复杂度
2. 兼容OpenAI接口，代码可复用
3. 网络稳定，适合墙内使用
4. 提供免费额度，适合DEMO验证

**第二步：设计统一接口**

参考VL的统一接口设计：
- 创建 `ILLMService` 接口
- 创建 `ApiLLMService` 实现（支持OpenAI兼容接口）
- 创建 `LLMServiceManager` 统一管理器
- 支持运行时切换服务（通义千问、DeepSeek等）

**第三步：成本控制**

- 使用免费额度进行DEMO验证
- 实现Token使用统计
- 优化Prompt长度
- 实现结果缓存

---

### 5.6 价格对比（2024年数据）

| 服务商 | LLM模型 | 输入价格（元/千Tokens） | VL模型 | 备注 |
|--------|---------|------------------------|--------|------|
| 通义千问 | Qwen-Plus | 0.012 | Qwen-VL | 兼容OpenAI ⭐ |
| 通义千问 | Qwen-Turbo | 0.002 | Qwen-VL | 快速验证 |
| DeepSeek | DeepSeek-V2 | 0.001 | DeepSeek-VL | 性价比最高 ⭐ |
| 文心一言 | ERNIE-Bot | 0.012 | ERNIE-Bot 5.0 | 全模态 |
| 智谱GLM | GLM-3 Turbo | 0.001 | - | 入门级 |

---

## 六、企业级混合部署方案（可选）

### 6.1 适用场景

**企业级大规模使用：**
- 大量测试用例并发执行
- 需要集中管理和监控
- 需要模型统一更新和维护

### 6.2 架构设计

```
┌─────────────────────────────────────────────────────────┐
│              企业级 LLM 服务器                             │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │ 用例理解 │  │ 屏幕理解 │  │ 操作规划 │              │
│  │ LLM      │  │ Vision-  │  │ LLM      │              │
│  │          │  │ Language │  │          │              │
│  └──────────┘  └──────────┘  └──────────┘              │
└────────────────────┬────────────────────────────────────┘
                     │ HTTP/WebSocket
                     │
┌────────────────────▼────────────────────────────────────┐
│           多个 TestWings APP (手机端)                    │
│  ┌──────────┐  ┌──────────┐                            │
│  │  OCR     │  │ UI检测   │                            │
│  │ PaddleOCR│  │ YOLOv8   │                            │
│  │ Mobile   │  │ Nano     │                            │
│  └──────────┘  └──────────┘                            │
└─────────────────────────────────────────────────────────┘
```

### 6.3 服务器部署

**硬件要求：**
- GPU：至少 1x NVIDIA A100 (40GB) 或 2x RTX 3090 (24GB)
- 内存：至少 32GB RAM
- 存储：至少 100GB（用于模型文件）

**部署方案：**
- 使用 vLLM 或 TensorRT-LLM 部署
- 提供 RESTful API 或 WebSocket 接口
- 支持并发请求和负载均衡

---

## 七、部署建议总结

### 7.1 优先方案：完全本地部署

**优势：**
- ✅ 降低测试环境复杂度，无需搭建服务器
- ✅ 保护数据隐私，不上传敏感信息
- ✅ 离线可用，不依赖网络
- ✅ 响应速度快，无网络延迟

**配置要求：**
- 8GB 内存：可以使用 Qwen-1.8B（轻量级）
- 12GB+ 内存：推荐使用 Qwen-7B（完整功能）

### 7.2 可选方案：API云端部署（DEMO验证）

**适用场景**：
- DEMO验证和快速开发
- 本地部署遇到技术限制
- 需要快速验证LLM+VL可行性

**推荐方案**：
- **主方案**：通义千问（LLM + VL，同一平台）
- **备选方案**：DeepSeek LLM + 通义千问VL（混合，成本最低）

**详细说明**：参见本章节"五、API云端部署方案"

---

### 7.3 可选方案：企业级混合部署

**适用场景：**
- 企业级大规模使用
- 需要集中管理
- 需要统一更新模型

**架构：**
- 本地：OCR + UI 检测（轻量级模型）
- 云端：LLM 服务（大模型）

---

**开发路线图**：参见 `02-02-开发路线图.md` - 了解模型部署的实施计划  
**当前问题分析**：参见 `docs/ONNX-Runtime-Android-zero-dimension限制导致Transformer模型past_key_values初始化失败问题分析.md`  
**下一步计划**：参见 `docs/02-02-开发路线图.md` - 了解详细的开发计划和优先级
