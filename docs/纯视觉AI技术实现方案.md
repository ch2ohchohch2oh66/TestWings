# 纯视觉AI技术实现方案

## 一、核心工作流程

### 1.1 主循环流程

```
开始测试
  ↓
预处理阶段（可选）
  ├─ 准备页面截图（静态或滚动截图）
  ├─ VL模型预处理识别所有元素
  └─ 存储预处理结果
  ↓
AI理解用例，生成测试计划
  ↓
循环执行（直到用例完成）：
  ├─ 捕获屏幕截图
  ├─ 匹配预处理结果
  │   ├─ 匹配成功：使用预处理结果
  │   └─ 未匹配：VL实时识别（OCR降级）
  ├─ LLM根据用例和屏幕状态制定策略
  ├─ LLM决定下一步操作
  ├─ 执行操作（点击/滑动/输入）
  ├─ 等待屏幕稳定
  ├─ 验证结果
  └─ 处理意外情况（如有）
  ↓
生成测试报告
```

### 1.2 单步决策流程

```
当前屏幕截图
  ↓
匹配预处理结果
  ├─ 匹配成功：使用预处理结果（快速）
  └─ 未匹配：VL实时识别
      ├─ VL可用：使用VL模型分析
      └─ VL不可用：OCR降级（可选）
  ↓
屏幕状态（预处理结果或实时识别）
  ├─ 识别所有文字
  ├─ 识别所有可操作元素（按钮、输入框等）
  ├─ 理解布局结构
  └─ 生成屏幕状态描述
  ↓
LLM策略规划
  ├─ 输入：用例步骤 + 屏幕状态
  ├─ 分析：当前应该做什么
  ├─ 决策：下一步操作
  └─ 输出：操作指令（类型、目标、参数）
  ↓
执行操作
  ↓
等待并验证
```

---

## 二、预处理机制集成（可选优化方案）

### 2.1 方案定位

**预处理机制是性能优化方案，不是必需功能**：
- ✅ 如果VL模型足够高效准确，可以完全依赖实时识别，不需要预处理
- ✅ 如果需要提升性能，可以使用预处理机制，减少实时识别请求
- ✅ 如果VL模型性能不足，预处理机制可以显著提升执行速度

### 2.2 预处理流程

**预处理阶段**：
1. 测试工程师准备截图（按简单命名规范：`APPNAME_页面名_场景名.png`）
2. **AI大模型自动识别**：VL模型分析截图，自动识别所有元素
3. **AI智能生成元数据**：LLM根据VL识别结果，智能生成元数据
4. 存储预处理结果（页面标识、元素列表、元数据）

**执行阶段**：
1. LLM理解用例中的页面描述（自然语言）
2. **智能匹配预处理截图**：
   - 文件名匹配（优先）
   - 语义匹配（LLM理解，主要能力）
   - 特征匹配（降级）
3. 匹配成功：直接使用预处理结果（快速）
4. 未匹配：VL实时识别（VL为主，OCR为辅）

**详细设计**：参见 `docs/预处理机制.md`

### 2.3 文件命名规范

**简单命名格式**：`APPNAME_页面名_场景名.png`

**示例**：
- `微信_登录页面.png`
- `微信_通讯录页面_空列表.png`
- `微信_我页面_设置_个人资料.png`

**特点**：
- ✅ 使用自然语言，符合日常使用习惯
- ✅ 无需版本号（AI自动识别页面变化）
- ✅ 无需复杂编码规则
- ✅ 测试工程师只需截图和简单命名，无需填写元数据

### 2.4 AI自动识别和元数据生成

**AI自动识别流程**：
1. VL模型分析截图 → 识别所有文字和UI元素
2. LLM智能生成元数据 → 根据VL识别结果，生成结构化元数据
3. 解析文件名 → 提取应用名称、页面名称、场景名称

**优势**：
- ✅ 完全自动化，测试工程师无需填写元数据
- ✅ 基于图像理解，准确率高
- ✅ 智能语义理解，生成准确的描述

---

## 三、Vision-Language模型集成

### 3.1 模型职责

**输入**：
- 屏幕截图（Bitmap）
- 当前测试步骤描述（可选）

**输出**（结构化JSON）：
```json
{
  "screen_description": "主屏幕，包含多个APP图标",
  "elements": [
    {
      "type": "button",
      "text": "捕获屏幕",
      "position": {"x": 588, "y": 1150, "width": 200, "height": 50},
      "confidence": 0.95
    },
    {
      "type": "text",
      "content": "TestWings - 自动化测试",
      "position": {"x": 100, "y": 50}
    }
  ],
  "layout": "垂直布局，包含标题、按钮、卡片等"
}
```

### 3.2 实现方式

**方案1：本地部署（推荐）**
- 模型：Qwen-VL（INT8量化，约4-5GB）
- 推理时间：2-5秒（预处理）或实时识别
- 优势：隐私安全，无需网络

**方案2：云端API（可选）**
- API：GPT-4V、Claude Vision
- 推理时间：1-3秒
- 优势：最新能力，无需本地部署

### 3.3 VL vs OCR策略

**核心策略**：VL为主，OCR为辅

**技术方案**：VL为主，OCR为辅

**Vision-Language模型特点**：
- 识别能力：文字 + UI元素
- 语义理解：支持语义理解
- 准确率：高
- 速度：2-5秒（本地VL模型）
- 资源占用：4-6GB内存
- 模型大小：4-5GB（INT8量化）

**OCR特点**（降级方案）：
- 识别能力：仅文字
- 速度：快（< 1秒）
- 资源占用：小（~100MB）
- 模型大小：小（~20MB）

**理由**：
1. **VL能力更强**：同时识别文字和UI元素，准确率更高
2. **符合纯视觉理念**：VL是纯视觉方案的核心
3. **预处理机制支持**：预处理使用VL，执行时直接使用结果，解决速度问题
4. **OCR作为降级**：仅在VL不可用时使用

**实施策略**：
- **预处理阶段**：使用VL模型预处理页面截图
- **执行阶段**：优先使用预处理结果（VL识别），未匹配时使用VL实时识别
- **降级方案**：OCR仅作为最后的降级方案（可选）

---

## 四、智能语义匹配

### 4.1 问题背景

在纯视觉AI自动化测试中，需要智能匹配目标元素：
- 用例中要求匹配："捕获屏幕"按钮
- Vision-Language模型识别结果：包含"捕获屏幕"、"埔获屏幕"等多个相似元素

传统的字符串匹配无法处理这种情况，需要语义理解能力。

### 4.2 核心思想

**Vision-Language模型在自动化测试中扮演关键角色**：
- VL模型能够同时理解图像和文本，进行多模态推理
- 能够理解"捕获屏幕"和"埔获屏幕"的语义相关性
- 提供智能匹配，提升测试成功率
- **VL为主，OCR为辅**：优先使用VL模型，OCR仅作为降级方案

### 4.3 匹配架构

```
┌─────────────────────────────────────────┐
│        智能语义匹配层                    │
├─────────────────────────────────────────┤
│ 第一层：VL模型语义匹配（主要能力）⭐      │
│  - VL模型识别所有元素及其语义描述          │
│  - 直接进行语义匹配                       │
│  - 响应时间：2-5秒（本地VL模型）          │
├─────────────────────────────────────────┤
│ 第二层：LLM辅助匹配（增强能力）           │
│  - 使用LLM理解目标描述                    │
│  - 匹配VL识别的元素                       │
│  - 响应时间：1-3秒（本地LLM）            │
├─────────────────────────────────────────┤
│ 第三层：OCR降级匹配（可选）              │
│  - 仅在VL模型不可用时使用                 │
│  - 直接匹配：OCR结果 = 目标文本          │
│  - 响应时间：< 1s                        │
└─────────────────────────────────────────┘
```

### 4.4 工作流程

**主要流程（VL模型可用）**：
```
1. 测试用例要求：点击"捕获屏幕"按钮
   ↓
2. Vision-Language模型理解屏幕
   - 输入：屏幕截图
   - VL模型识别：所有UI元素（文字、按钮、输入框等）及其语义描述
   - 输出：元素列表，包含"捕获屏幕"按钮（语义描述匹配）
   ↓
3. VL语义匹配：成功（VL模型已识别到匹配元素）
   - 元素：按钮，文本="捕获屏幕"，坐标=(540, 960)
   - 置信度：0.95
   ↓
4. 返回匹配结果：成功定位到"捕获屏幕"按钮
```

**降级流程（VL模型不可用时）**：
```
1. 测试用例要求：点击"捕获屏幕"按钮
   ↓
2. OCR识别结果：["埔获屏幕", "点击中心", "刷新"]
   ↓
3. 直接匹配：失败（"捕获屏幕" ≠ "埔获屏幕"）
   ↓
4. LLM语义匹配：使用LLM理解语义相关性
   - 输入：目标文本="捕获屏幕"，OCR结果=["埔获屏幕", ...]
   - LLM分析：理解"捕获屏幕"和"埔获屏幕"的语义相关性
   - 输出：匹配成功，置信度=0.85
   ↓
5. 返回匹配结果：成功定位到"埔获屏幕"按钮
```

### 4.5 实现方式

**API设计**：
```kotlin
suspend fun matchElement(
    targetDescription: String,        // 目标描述（来自测试用例）
    screenState: ScreenState,        // VL模型识别的屏幕状态
    useVlMatch: Boolean = true,       // 是否使用VL模型匹配（优先）
    useOcrFallback: Boolean = false  // 是否使用OCR降级方案
): MatchResult
```

**自动选择逻辑**：
```kotlin
suspend fun matchElement(
    targetDescription: String, 
    screenState: ScreenState
): MatchResult {
    // 第一步：优先使用VL模型匹配（主要能力）
    if (screenState.vlAvailable) {
        val vlMatch = tryVlMatch(targetDescription, screenState)
        if (vlMatch.success) {
            return vlMatch  // VL匹配成功，直接返回
        }
    }
    
    // 第二步：VL匹配失败，使用LLM辅助匹配
    val llmMatch = tryLlmMatch(targetDescription, screenState)
    if (llmMatch.success) {
        return llmMatch
    }
    
    // 第三步：降级到OCR匹配（可选）
    if (screenState.ocrFallback) {
        return tryOcrMatch(targetDescription, screenState.ocrResult)
    }
    
    return MatchResult(success = false)
}
```

### 4.6 性能考虑

| 匹配方式 | 响应时间 | 适用场景 |
|---------|---------|---------|
| VL模型匹配（主要）⭐ | 2-5秒 | 主要能力，理解屏幕语义 |
| LLM辅助匹配 | 1-3秒 | VL匹配失败时，增强语义理解 |
| OCR降级匹配 | < 1s | VL不可用时，简单文本匹配 |

---

## 五、LLM策略规划集成

### 3.1 模型职责

**输入**（Prompt）：
```
你是一个自动化测试助手。请根据测试用例和当前屏幕状态，决定下一步操作。

测试用例步骤：打开"微信"APP并登录
当前屏幕状态：[Vision-Language模型的输出]
历史操作：[已执行的操作列表]

请分析：
1. 当前屏幕是否满足用例要求？
2. 如果不满足，下一步应该做什么？
3. 具体操作是什么？（操作类型、目标元素、参数）

请返回JSON格式的操作指令。
```

**输出**（JSON）：
```json
{
  "action": "click",
  "target": {
    "type": "app_icon",
    "text": "微信",
    "position": {"x": 100, "y": 200}
  },
  "reason": "找到微信APP图标，点击打开",
  "confidence": 0.9
}
```

### 3.2 实现方式

**方案1：本地部署（推荐）**
- 模型：Qwen-7B（INT8量化，约4-5GB）
- 推理时间：2-5秒
- 优势：隐私安全，无需网络

**方案2：云端API（可选）**
- API：GPT-4、Claude
- 推理时间：1-3秒
- 优势：最新能力，无需本地部署

---

## 六、操作执行层

### 6.1 纯视觉定位

**定位方式**：
- 屏幕坐标（从Vision-Language模型获取）
- 元素位置（从AI识别结果获取）

### 6.2 操作类型

1. **点击操作**
   - 输入：目标元素的位置坐标
   - 执行：使用坐标点击

2. **滑动操作**
   - 输入：起始坐标、结束坐标、滑动距离
   - 执行：使用坐标滑动

3. **输入操作**
   - 输入：目标输入框位置、输入文本
   - 执行：点击输入框 → 输入文本

4. **等待操作**
   - 输入：等待时间或等待条件
   - 执行：等待屏幕稳定

---

## 七、意外情况处理

### 7.1 处理策略

**核心思想**：AI自主决策，不硬编码

**流程**：
```
检测到意外情况（弹窗、广告等）
  ↓
Vision-Language模型识别
  ↓
LLM分析情况
  ├─ 这是什么情况？
  ├─ 对测试用例的影响？
  └─ 应该如何处理？
  ↓
LLM决定处理方式
  ├─ 点击"跳过"按钮
  ├─ 点击"X"关闭
  ├─ 点击"允许"权限
  └─ 或其他处理方式
  ↓
执行处理操作
  ↓
继续测试流程
```

### 7.2 典型场景（示例）

1. **广告页面**：AI识别广告特征 → 查找关闭按钮 → 点击关闭
2. **更新弹窗**：AI识别更新弹窗 → 根据用例决定更新/忽略
3. **权限弹窗**：AI识别权限请求 → 根据测试场景决定允许/拒绝
4. **意外弹窗**：AI识别弹窗内容 → 根据测试场景智能决策

**注意**：这些场景的处理完全由AI自主决策，不需要硬编码逻辑。

---

## 八、用例格式支持

### 8.1 自然语言用例

**示例**：
```
打开微信APP，登录账号，进入聊天界面，发送消息"你好"给联系人"张三"
```

**处理流程**：
1. LLM理解用例意图
2. 生成结构化测试计划
3. 逐步执行

### 8.2 结构化用例（兼容）

**示例**（JSON）：
```json
{
  "name": "微信登录测试",
  "steps": [
    {"action": "open_app", "app": "微信"},
    {"action": "login", "username": "xxx", "password": "xxx"}
  ]
}
```

**处理流程**：
1. 直接解析结构化用例
2. 或转换为自然语言后由AI理解

---

## 九、性能优化

### 9.1 预处理机制优化

**方案定位**：
- 预处理机制是性能优化方案，不是必需功能
- 如果VL模型足够高效准确，可以不需要预处理
- 如果需要提升性能，可以使用预处理机制

**优势**：
- 预处理使用VL，执行时直接使用结果，避免实时识别
- 完整页面截图，识别更全面，准确率更高
- 减少实时计算开销
- AI自动识别和生成元数据，测试工程师工作量低

### 9.2 响应时间优化

1. **预处理机制**：预处理使用VL，执行时直接使用结果
2. **模型预加载**：启动时预加载模型
3. **并行处理**：屏幕捕获和模型推理并行
4. **缓存机制**：缓存预处理结果和常见场景的识别结果
5. **模型量化**：使用INT8量化减少推理时间

### 9.3 资源占用优化

1. **模型共享**：多个任务共享同一个模型实例
2. **按需加载**：根据设备性能选择模型大小
3. **内存管理**：及时释放不需要的资源

---

## 十、实施优先级

### Phase 1: 基础能力（当前）
- ✅ 屏幕捕获
- ✅ OCR识别
- ✅ 基础操作执行

### Phase 2: Vision-Language集成
- ⏳ 集成Qwen-VL模型
- ⏳ 实现屏幕内容理解
- ⏳ 实现元素识别和定位
- ⏳ 实现预处理机制（可选，提升性能）

### Phase 3: LLM策略规划
- ⏳ 集成Qwen-7B模型
- ⏳ 实现用例理解
- ⏳ 实现策略规划
- ⏳ 实现自主决策

### Phase 4: 完整流程
- ⏳ 支持自然语言用例
- ⏳ 实现完整测试流程
- ⏳ 实现意外情况处理
- ⏳ 优化性能和准确率

---

## 十一、关键技术点

### 11.1 模型选择

**Vision-Language**：
- 优先：Qwen-VL（本地）
- 备选：GPT-4V、Claude Vision（云端）

**LLM**：
- 优先：Qwen-7B（本地）
- 备选：GPT-4、Claude（云端）

### 11.2 部署方案

**完全本地部署（推荐）**：
- 所有模型本地部署
- 隐私安全，无需网络
- 适合企业内网环境

**混合部署（可选）**：
- 轻量级模型本地
- 大模型云端
- 适合需要最新能力的场景

---

## 十二、总结

### 12.1 核心能力

1. **纯视觉操作**：完全基于屏幕截图
2. **AI自主决策**：大模型参与每一步决策
3. **自然语言用例**：支持自然语言描述
4. **智能容错**：能够处理意外情况

---

**下一步**：开始实施Phase 2，集成Vision-Language模型，实现预处理机制。

**相关文档**：
- `docs/预处理机制.md` - 预处理机制详细设计
- `docs/架构设计.md` - 系统架构设计

