# 纯视觉AI技术实现方案

## 一、核心工作流程

### 1.1 主循环流程

```
开始测试
  ↓
预处理阶段（可选）
  ├─ 准备页面截图（静态或滚动截图）
  ├─ VL模型预处理识别所有元素
  └─ 存储预处理结果
  ↓
AI理解用例，生成测试计划
  ↓
循环执行（直到用例完成）：
  ├─ 捕获屏幕截图
  ├─ 匹配预处理结果
  │   ├─ 匹配成功：使用预处理结果
  │   └─ 未匹配：VL实时识别（OCR降级）
  ├─ LLM根据用例和屏幕状态制定策略
  ├─ LLM决定下一步操作
  ├─ 执行操作（点击/滑动/输入）
  ├─ 等待屏幕稳定
  ├─ 验证结果
  └─ 处理意外情况（如有）
  ↓
生成测试报告
```

### 1.2 单步决策流程

```
当前屏幕截图
  ↓
匹配预处理结果
  ├─ 匹配成功：使用预处理结果（快速）
  └─ 未匹配：VL实时识别
      ├─ VL可用：使用VL模型分析
      └─ VL不可用：OCR降级（可选）
  ↓
屏幕状态（预处理结果或实时识别）
  ├─ 识别所有文字
  ├─ 识别所有可操作元素（按钮、输入框等）
  ├─ 理解布局结构
  └─ 生成屏幕状态描述
  ↓
LLM策略规划
  ├─ 输入：用例步骤 + 屏幕状态
  ├─ 分析：当前应该做什么
  ├─ 决策：下一步操作
  └─ 输出：操作指令（类型、目标、参数）
  ↓
执行操作
  ↓
等待并验证
```

---

## 二、预处理机制集成

### 2.1 预处理流程

**预处理阶段**：
1. 准备页面截图（静态页面或滚动截图）
2. VL模型预处理，识别所有元素
3. 存储预处理结果（页面标识、元素列表）

**执行阶段**：
1. 匹配当前屏幕与预处理页面
2. 匹配成功：直接使用预处理结果
3. 未匹配：VL实时识别（OCR降级）

**详细设计**：参见 `docs/预处理机制设计.md`

### 2.2 滚动截图处理

**方式**：
- 自动滚动并截图
- 拼接多张截图
- 生成完整页面截图

**优势**：
- 识别完整页面内容
- 包含所有可滚动元素
- 提高元素定位准确率

---

## 三、Vision-Language模型集成

### 3.1 模型职责

**输入**：
- 屏幕截图（Bitmap）
- 当前测试步骤描述（可选）

**输出**（结构化JSON）：
```json
{
  "screen_description": "主屏幕，包含多个APP图标",
  "elements": [
    {
      "type": "button",
      "text": "捕获屏幕",
      "position": {"x": 588, "y": 1150, "width": 200, "height": 50},
      "confidence": 0.95
    },
    {
      "type": "text",
      "content": "TestWings - 自动化测试",
      "position": {"x": 100, "y": 50}
    }
  ],
  "layout": "垂直布局，包含标题、按钮、卡片等"
}
```

### 3.2 实现方式

**方案1：本地部署（推荐）**
- 模型：Qwen-VL（INT8量化，约4-5GB）
- 推理时间：2-5秒（预处理）或实时识别
- 优势：隐私安全，无需网络

**方案2：云端API（可选）**
- API：GPT-4V、Claude Vision
- 推理时间：1-3秒
- 优势：最新能力，无需本地部署

### 3.3 VL vs OCR策略

**核心策略**：VL为主，OCR为辅

**理由**：
- VL能力更强：同时识别文字和UI元素
- VL准确率更高：理解语义和上下文
- 预处理机制解决速度问题：预处理使用VL，执行时直接使用结果
- OCR作为降级：仅在VL不可用时使用

**详细分析**：参见 `docs/OCR vs VL选择分析.md`

---

## 四、LLM策略规划集成

### 3.1 模型职责

**输入**（Prompt）：
```
你是一个自动化测试助手。请根据测试用例和当前屏幕状态，决定下一步操作。

测试用例步骤：打开"微信"APP并登录
当前屏幕状态：[Vision-Language模型的输出]
历史操作：[已执行的操作列表]

请分析：
1. 当前屏幕是否满足用例要求？
2. 如果不满足，下一步应该做什么？
3. 具体操作是什么？（操作类型、目标元素、参数）

请返回JSON格式的操作指令。
```

**输出**（JSON）：
```json
{
  "action": "click",
  "target": {
    "type": "app_icon",
    "text": "微信",
    "position": {"x": 100, "y": 200}
  },
  "reason": "找到微信APP图标，点击打开",
  "confidence": 0.9
}
```

### 3.2 实现方式

**方案1：本地部署（推荐）**
- 模型：Qwen-7B（INT8量化，约4-5GB）
- 推理时间：2-5秒
- 优势：隐私安全，无需网络

**方案2：云端API（可选）**
- API：GPT-4、Claude
- 推理时间：1-3秒
- 优势：最新能力，无需本地部署

---

## 五、操作执行层

### 4.1 纯视觉定位

**不再依赖**：
- ❌ Accessibility Service
- ❌ Resource ID
- ❌ UI层级结构

**完全基于**：
- ✅ 屏幕坐标（从Vision-Language模型获取）
- ✅ 元素位置（从AI识别结果获取）

### 4.2 操作类型

1. **点击操作**
   - 输入：目标元素的位置坐标
   - 执行：使用坐标点击

2. **滑动操作**
   - 输入：起始坐标、结束坐标、滑动距离
   - 执行：使用坐标滑动

3. **输入操作**
   - 输入：目标输入框位置、输入文本
   - 执行：点击输入框 → 输入文本

4. **等待操作**
   - 输入：等待时间或等待条件
   - 执行：等待屏幕稳定

---

## 六、意外情况处理

### 5.1 处理策略

**核心思想**：AI自主决策，不硬编码

**流程**：
```
检测到意外情况（弹窗、广告等）
  ↓
Vision-Language模型识别
  ↓
LLM分析情况
  ├─ 这是什么情况？
  ├─ 对测试用例的影响？
  └─ 应该如何处理？
  ↓
LLM决定处理方式
  ├─ 点击"跳过"按钮
  ├─ 点击"X"关闭
  ├─ 点击"允许"权限
  └─ 或其他处理方式
  ↓
执行处理操作
  ↓
继续测试流程
```

### 5.2 典型场景（示例）

1. **广告页面**：AI识别广告特征 → 查找关闭按钮 → 点击关闭
2. **更新弹窗**：AI识别更新弹窗 → 根据用例决定更新/忽略
3. **权限弹窗**：AI识别权限请求 → 根据测试场景决定允许/拒绝
4. **意外弹窗**：AI识别弹窗内容 → 根据测试场景智能决策

**注意**：这些场景的处理完全由AI自主决策，不需要硬编码逻辑。

---

## 七、用例格式支持

### 6.1 自然语言用例

**示例**：
```
打开微信APP，登录账号，进入聊天界面，发送消息"你好"给联系人"张三"
```

**处理流程**：
1. LLM理解用例意图
2. 生成结构化测试计划
3. 逐步执行

### 6.2 结构化用例（兼容）

**示例**（JSON）：
```json
{
  "name": "微信登录测试",
  "steps": [
    {"action": "open_app", "app": "微信"},
    {"action": "login", "username": "xxx", "password": "xxx"}
  ]
}
```

**处理流程**：
1. 直接解析结构化用例
2. 或转换为自然语言后由AI理解

---

## 八、性能优化

### 8.1 预处理机制优化

**优势**：
- 预处理使用VL，执行时直接使用结果，避免实时识别
- 完整页面截图，识别更全面，准确率更高
- 减少实时计算开销

### 8.2 响应时间优化

1. **预处理机制**：预处理使用VL，执行时直接使用结果
2. **模型预加载**：启动时预加载模型
3. **并行处理**：屏幕捕获和模型推理并行
4. **缓存机制**：缓存预处理结果和常见场景的识别结果
5. **模型量化**：使用INT8量化减少推理时间

### 8.3 资源占用优化

1. **模型共享**：多个任务共享同一个模型实例
2. **按需加载**：根据设备性能选择模型大小
3. **内存管理**：及时释放不需要的资源

---

## 九、实施优先级

### Phase 1: 基础能力（当前）
- ✅ 屏幕捕获
- ✅ OCR识别
- ✅ 基础操作执行

### Phase 2: Vision-Language集成
- ⏳ 集成Qwen-VL模型
- ⏳ 实现屏幕内容理解
- ⏳ 实现元素识别和定位
- ⏳ 实现预处理机制（可选，提升性能）

### Phase 3: LLM策略规划
- ⏳ 集成Qwen-7B模型
- ⏳ 实现用例理解
- ⏳ 实现策略规划
- ⏳ 实现自主决策

### Phase 4: 完整流程
- ⏳ 支持自然语言用例
- ⏳ 实现完整测试流程
- ⏳ 实现意外情况处理
- ⏳ 优化性能和准确率

---

## 十、关键技术点

### 9.1 模型选择

**Vision-Language**：
- 优先：Qwen-VL（本地）
- 备选：GPT-4V、Claude Vision（云端）

**LLM**：
- 优先：Qwen-7B（本地）
- 备选：GPT-4、Claude（云端）

### 9.2 部署方案

**完全本地部署（推荐）**：
- 所有模型本地部署
- 隐私安全，无需网络
- 适合企业内网环境

**混合部署（可选）**：
- 轻量级模型本地
- 大模型云端
- 适合需要最新能力的场景

---

## 十一、总结

### 10.1 核心转变

**从**：OCR + Accessibility Service + 硬编码逻辑
**到**：Vision-Language + LLM + AI自主决策

### 10.2 关键能力

1. **纯视觉操作**：完全基于屏幕截图
2. **AI自主决策**：大模型参与每一步决策
3. **自然语言用例**：支持自然语言描述
4. **智能容错**：能够处理意外情况

---

**下一步**：开始实施Phase 2，集成Vision-Language模型，实现预处理机制。

**相关文档**：
- `docs/预处理机制设计.md` - 预处理机制详细设计
- `docs/OCR vs VL选择分析.md` - VL vs OCR选择分析

