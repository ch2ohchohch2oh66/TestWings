# TestWings 架构设计文档

> **文档定位**：本文档从**系统架构**角度，描述 TestWings 的整体设计、分层架构、技术选型、数据模型和部署方案。适合了解系统的整体结构和设计思路。

> **相关文档**：
> - `01-02-纯视觉AI技术实现方案.md` - 纯视觉AI技术的详细实现方案（技术细节）
> - `01-03-模型部署方案.md` - AI模型部署的具体方案
> - `01-04-预处理机制.md` - 预处理机制的详细设计

## 一、核心设计理念

### 1.1 设计目标

**TestWings 是一个完全基于视觉AI的自动化测试框架**，核心特点：

1. **AI视觉理解**：
   - **LLM理解自然语言用例**：理解测试用例的意图和执行状态，制定操作策略
   - **VL/大模型识别屏幕元素**：识别屏幕上的文字、按钮、输入框等所有可操作元素
   - **AI自主决策**：根据用例和屏幕状态，自主规划下一步操作
2. **类人操作**：像人一样观察屏幕、理解内容、做出决策、执行操作
3. **操作执行策略**：
   - **最优方式**：Accessibility Service（如果可用）
     - **准确性**：100%（直接操作控件节点）
     - **高效性**：<0.01秒（不需要VL识别）
     - **稳定性**：最高（不依赖坐标，直接操作控件）
     - **工程实践评估**：这是普通应用唯一可行的自动化操作方案，且用户只需一次授权
   - **降级方式**：纯坐标操作（Accessibility Service不可用时）
     - **准确性**：90-95%（依赖VL识别准确性）
     - **高效性**：2-5秒（需要VL模型推理）
     - **稳定性**：中等（依赖坐标准确性）
     - **适用场景**：Accessibility Service不可用时的降级方案
4. **实时响应**：实时获取屏幕信息，实时决策，实时执行

### 1.2 核心能力

- **视觉理解**：理解屏幕上的所有元素（文字、按钮、图标、布局等）
- **语义理解**：理解用例意图和屏幕内容的语义
- **策略规划**：根据用例和屏幕状态，制定操作步骤
- **自主决策**：像人一样理解任何屏幕情况，自主决策处理方式（不需要穷举场景）
- **操作执行**：将决策转化为具体的操作（点击、滑动、输入等）
- **预处理机制**（必需的性能优化，从工程实践角度）：
  - **性能提升**：将响应时间从2-5秒降低到<0.1秒，提升执行效率10-50倍
  - **准确性提升**：预处理使用完整页面截图，识别更全面，准确率更高
  - **成本**：预处理只需一次，后续直接使用结果，成本低
  - **工程实践结论**：这是必需的性能优化，不是可选功能

### 1.3 典型场景

- **打开指定APP**：AI滑动查找，找到后点击
- **处理广告**：AI识别并自动关闭
- **处理更新弹窗**：AI根据用例要求决策
- **处理权限弹窗**：AI智能识别并决策
- **处理意外弹窗**：AI根据场景智能决策

**注意**：这些场景由AI自主处理，不硬编码逻辑。

### 1.4 技术原理

**纯视觉AI自动化测试的核心原理**：

1. **屏幕内容识别**：
   - 屏幕截图 → Vision-Language模型理解 → 识别所有元素（文字、按钮、输入框等）
   - VL为主，OCR为辅（降级方案）

2. **任务理解与规划**：
   - 用户指令（自然语言） → LLM理解意图 → 任务分解 → 操作序列生成

3. **执行控制**：
   - 操作指令 → VL模型元素定位（纯视觉） → 操作执行（纯坐标） → 结果验证 → 下一步决策

4. **数据流转**：
   - **最优方案**：完全本地部署
     - 所有AI模型在手机本地运行
     - **优势**：无网络延迟、隐私安全、离线可用、无API费用
     - **工程实践评估**：这是最优方案，除非有特殊需求
   - **备选方案**：混合部署（不推荐，除非有特殊需求）
     - 轻量级模型本地，大模型云端
     - **劣势**：增加网络延迟、隐私风险、API费用、依赖网络
     - **适用场景**：需要最新能力，且可以接受上述劣势

---

## 二、整体架构

### 2.1 系统分层架构

```
┌─────────────────────────────────────────────────────────────┐
│                      用户接口层                               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  Web管理端   │  │  移动端APP   │  │  CLI命令行   │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      测试用例管理层                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  用例解析    │  │  用例存储    │  │  用例执行    │      │
│  │  (Parser)    │  │  (Database)  │  │  (Executor)  │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      AI核心引擎层                            │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  用例理解    │  │  屏幕理解    │  │  操作规划    │      │
│  │  (LLM)       │  │  (Vision+OCR)│  │  (LLM)       │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      设备控制层                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  屏幕捕获    │  │  元素定位    │  │  操作执行    │      │
│  │  (Capture)   │  │  (Locator)   │  │  (Executor)  │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└────────────────────┬───────────────────────────────────────┘
                     │
┌────────────────────▼───────────────────────────────────────┐
│                      结果分析层                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  结果验证    │  │  报告生成    │  │  问题分析    │      │
│  │  (Verifier)  │  │  (Reporter)  │  │  (Analyzer)  │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 核心模块说明

#### 2.2.1 用户接口层
- **Web管理端**：提供测试用例管理、执行监控、报告查看等功能
- **移动端APP**：作为测试执行代理，安装在测试设备上
- **CLI命令行**：提供命令行工具，支持CI/CD集成

#### 2.2.2 测试用例管理层
- **用例解析**：支持多种格式（自然语言、JSON、YAML、Python脚本）
- **用例存储**：用例数据库，支持版本管理
- **用例执行**：测试执行引擎，支持并行执行

#### 2.2.3 AI核心引擎层
- **用例理解**：使用LLM理解测试用例意图（自然语言）
- **屏幕理解**：使用Vision-Language模型理解屏幕内容（VL为主，OCR为辅）
- **操作规划**：LLM像人一样理解屏幕情况，自主规划操作（通用理解，不穷举场景）
  - **核心能力**：LLM具备通用理解能力，能够理解任何屏幕情况并做出决策
  - **实时决策**：根据当前屏幕的实际内容，像人一样分析并决策
  - **灵活应对**：能够处理从未见过的场景，适应各种变化

#### 2.2.4 设备控制层
- **屏幕捕获**：实时获取设备屏幕截图（MediaProjection API）
- **元素定位**：参见 `01-02-纯视觉AI技术实现方案.md` - 六、操作执行层（6.1元素定位策略）
  - **主要方式**：VL综合定位（90-95%准确率，预处理后<0.1秒）
  - **增强方式**：VL识别 + Accessibility Service验证（如果可用）
  - **快速路径**：RESOURCE_ID定位（如果已知Resource ID，100%准确率，<0.01秒）
  - **降级方案**：TEXT定位（仅文本匹配，80-90%准确率）
- **操作执行**：参见 `01-02-纯视觉AI技术实现方案.md` - 六、操作执行层（6.2操作执行方式、6.3操作类型）
  - **最优方式**：Accessibility Service（如果可用，100%准确率，<0.01秒，最稳定）
  - **降级方式**：纯坐标操作（Accessibility Service不可用时，90-95%准确率，2-5秒）

#### 2.2.5 结果分析层
- **结果验证**：验证测试步骤执行结果
- **报告生成**：生成测试报告（HTML、JSON、PDF）
- **问题分析**：分析失败原因，提供修复建议

---

## 三、核心流程设计

**详细的工作流程**：参见 `01-02-纯视觉AI技术实现方案.md` - 一、核心工作流程

**流程概述**：
- **测试执行主流程**：理解用例 → 执行步骤 → 验证结果 → 生成报告
- **AI理解流程**：用例理解（LLM）→ 屏幕理解（VL）→ 操作规划（LLM）

---

## 四、技术栈选型

### 4.1 移动端（Android/iOS）

#### Android
- **屏幕捕获**：MediaProjection API（必需，唯一可行的屏幕捕获方案）
- **屏幕理解**：Vision-Language模型（Qwen-VL等，纯视觉，必需）
- **操作执行**：
  - **最优方式**：Accessibility Service（普通应用唯一可行的自动化操作方案）
    - **准确性**：100%，**高效性**：<0.01秒，**稳定性**：最高
    - 用户只需一次授权开启，之后可以完全自动化
    - 支持直接操作节点（performAction，最优）或坐标操作（getBoundsInScreen + clickAt，备选）
    - **注意**：Instrumentation API和UiAutomation API需要系统权限，普通应用无法使用
  - **降级方式**：纯坐标操作（Accessibility Service不可用时）
    - **准确性**：90-95%，**高效性**：2-5秒（需要VL识别），**稳定性**：中等
    - 基于VL模型识别的控件中心坐标
- **OCR**：ML Kit Text Recognition（降级方案，仅在VL不可用时使用）
- **图像处理**：OpenCV for Android（可选，用于图像预处理和特征提取）

#### iOS
- **屏幕捕获**：ScreenCapture API
- **UI访问**：XCUITest / Accessibility API
- **操作执行**：XCUITest / 私有API（需越狱）
- **OCR**：Vision Framework / PaddleOCR
- **图像处理**：Core Image / OpenCV

### 4.2 AI模型选型

**详细的模型选型、部署方案和工程实践评估**：参见 `01-03-模型部署方案.md`

**简要说明**：
- **VL模型**：Qwen-VL-Chat（本地部署，核心能力）
- **LLM模型**：Qwen-7B-Chat（本地部署，用例理解和操作规划）
- **OCR**：ML Kit Text Recognition（降级方案）
- **部署方案**：完全本地部署（最优方案）

### 4.3 数据存储（完全本地）

- **数据库**：SQLite（用例存储、执行结果）
- **文件存储**：本地文件系统（截图、日志、报告）
- **缓存**：内存缓存（模型推理结果、UI 元素识别结果）

### 4.4 企业级后端服务（可选）

**仅在企业级混合部署场景需要：**
- **Web框架**：FastAPI / Flask
- **数据库**：PostgreSQL（用例存储）、Redis（缓存）
- **消息队列**：RabbitMQ / Kafka（异步任务）
- **任务调度**：Celery
- **API网关**：Nginx
- **LLM服务**：vLLM、TensorRT-LLM

### 4.5 移动端APP

- **开发框架**：Android 原生（Kotlin/Java）
- **UI框架**：Jetpack Compose 或传统 View 系统
- **数据库**：Room（SQLite 封装）
- **异步处理**：Kotlin Coroutines

---

## 五、数据模型设计

### 5.1 测试用例模型

```python
class TestCase:
    id: str
    name: str
    description: str
    format: str  # 'natural_language', 'json', 'yaml', 'python'
    content: str  # 原始用例内容
    parsed_content: dict  # 解析后的结构化内容
    app_package: str  # 目标应用包名
    created_at: datetime
    updated_at: datetime
    version: int

class TestStep:
    step_id: int
    action: str  # 'click', 'input', 'swipe', 'verify', 'wait'
    target: str  # 目标元素描述
    params: dict  # 操作参数
    expected: dict  # 预期结果
    timeout: int  # 超时时间（秒）
```

### 5.2 屏幕状态模型

```python
class ScreenState:
    screenshot_path: str
    timestamp: datetime
    elements: List[UIElement]  # VL模型识别的所有元素（文字、按钮、输入框等）
    semantic_description: str  # VL模型生成的语义描述
    decision: Decision  # LLM的决策结果

class TextElement:
    text: str
    bbox: BoundingBox  # 边界框
    confidence: float

class UIElement:
    type: str  # 'button', 'input', 'image', 'text'
    bbox: BoundingBox
    attributes: dict  # 属性（颜色、大小等）
    confidence: float
```

### 5.3 执行结果模型

```python
class TestExecution:
    id: str
    test_case_id: str
    device_id: str
    status: str  # 'running', 'passed', 'failed', 'error'
    start_time: datetime
    end_time: datetime
    steps: List[StepExecution]
    screenshots: List[str]
    report_path: str

class StepExecution:
    step_id: int
    action: str
    target: str
    status: str  # 'success', 'failed', 'skipped'
    execution_time: float
    screenshot_before: str
    screenshot_after: str
    error_message: str
    verification_result: VerificationResult
```

---

## 六、关键算法设计

**详细的算法实现**：参见 `01-02-纯视觉AI技术实现方案.md`
- **元素定位算法**：参见 四、智能语义匹配、六、操作执行层（6.1元素定位策略）
- **异常处理策略**：参见 七、意外情况处理

---

## 七、框架方案选择

### 7.1 为什么选择 APP 形式？

**核心原因：**

1. **系统权限需求**
   - 屏幕捕获需要 `MediaProjection` API，需要应用权限
   - 纯视觉操作，不依赖 `AccessibilityService`
   - 这些系统级权限无法通过其他方式获得

2. **本地 AI 模型部署**
   - AI 模型需要在设备本地运行
   - APP 形式可以完整集成 OCR、UI 检测、LLM 等模型
   - 便于模型管理和更新

3. **用户体验**
   - 提供友好的用户界面（管理用例、查看报告）
   - 可以独立运行，不依赖 PC
   - 支持离线使用

4. **开发便利性**
   - Android SDK 提供完整的开发工具链
   - 调试和测试方便
   - 社区资源丰富

### 7.2 实现方案

**采用：标准 Android APP 形式**

**结构：**
- 标准 Android APP
- 使用 Jetpack Compose UI 框架
- 完整的用户界面

---

## 八、部署架构

**详细的部署架构方案**：参见 `01-03-模型部署方案.md` - 了解完全本地部署和企业级混合部署的详细方案

---

## 九、性能优化策略

**详细的性能优化方案**：参见 `01-02-纯视觉AI技术实现方案.md` - 九、性能优化

**优化策略概述**：
- **预处理机制优化**：必需的性能优化，提升执行效率10-50倍
- **响应时间优化**：预处理、模型预加载、并行处理、缓存机制、模型量化
- **资源占用优化**：模型共享、按需加载、内存管理

---

## 十、安全与隐私

### 10.1 数据安全

1. **加密传输**：所有数据传输使用HTTPS/WSS
2. **数据加密存储**：敏感数据加密存储
3. **访问控制**：基于角色的访问控制（RBAC）

### 10.2 隐私保护

1. **本地处理优先**：优先使用本地模型，减少数据上传
2. **数据脱敏**：上传前对敏感信息脱敏
3. **用户控制**：用户可控制数据上传范围
4. **数据保留策略**：自动清理过期数据

---

## 十一、扩展性设计

### 11.1 插件系统

支持插件扩展功能：
- **自定义识别器**：支持自定义UI元素识别器
- **自定义操作**：支持自定义操作类型
- **自定义验证器**：支持自定义验证逻辑

### 11.2 多平台支持

- **Android**：完整支持
- **iOS**：基础支持（受系统限制）
- **Web**：通过浏览器自动化扩展支持

### 11.3 多语言支持

- **测试用例**：支持多语言测试用例
- **UI识别**：支持多语言UI识别
- **报告生成**：支持多语言报告

---

## 十三、总结

### 13.1 核心价值

1. **纯视觉操作**：完全基于屏幕截图，不依赖系统服务
2. **AI自主决策**：大模型参与每一步决策，像人一样思考
3. **自然语言用例**：支持自然语言描述测试用例
4. **智能容错**：能够处理意外情况，智能决策

### 13.2 核心特点

**方案特点**：Vision-Language + LLM + AI自主决策 + 纯视觉操作

---

**详细技术实现**：参见 `01-02-纯视觉AI技术实现方案.md`
**预处理机制**：参见 `01-04-预处理机制.md`
**开发路线图**：参见 `02-02-开发路线图.md`

